import os
import time
import datetime
from typing import Optional, List, Dict, Any
from functools import wraps

import fire
import pandas as pd
import numpy as np
import tushare as ts
from sqlalchemy import create_engine, text
from sqlalchemy import Table, MetaData
from sqlalchemy.dialects.mysql import insert as mysql_insert
import pymysql  # noqa: F401 - required by SQLAlchemy URL


# Tushare init
ts.set_token(os.environ["TUSHARE"])  # expects env var set
pro = ts.pro_api()


def retry_on_failure(max_retries: int = 3, delay: float = 1.0, backoff: float = 2.0):
    """
    Decorator: Add retry mechanism to function

    Args:
        max_retries: Maximum number of retries
        delay: Initial delay time (seconds)
        backoff: Delay multiplier
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            current_delay = delay

            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e

                    if attempt < max_retries:
                        print(f"Function {func.__name__} call failed (attempt {attempt + 1}/{max_retries + 1}): {e}")
                        print(f"Waiting {current_delay:.1f} seconds before retry...")
                        time.sleep(current_delay)
                        current_delay *= backoff  # exponential backoff
                    else:
                        print(f"Function {func.__name__} failed after {max_retries + 1} attempts: {e}")
                        raise last_exception

            # This line won't be executed, but for type checker
            raise last_exception

        return wrapper
    return decorator


def call_tushare_api_with_retry(api_func, *args, **kwargs):
    """
    Generic Tushare API call function with retry mechanism

    Args:
        api_func: Tushare API function
        *args: Positional arguments
        **kwargs: Keyword arguments

    Returns:
        DataFrame returned by API
    """
    @retry_on_failure(max_retries=3, delay=1.0, backoff=2.0)
    def _call_api():
        return api_func(*args, **kwargs)

    return _call_api()


TABLE_NAME = "ts_a_stock_financial_profile"


CREATE_TABLE_DDL = f"""
CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
  ts_code                   VARCHAR(16)  NOT NULL,
  report_period             DATE         NOT NULL,
  period                    VARCHAR(8)   NOT NULL,
  currency                  VARCHAR(3)   NOT NULL,
  ann_date                  DATE         NULL,

  -- Income statement fields (ä¸‡å…ƒå­˜å‚¨ - converted from å…ƒ)
  basic_eps                 FLOAT NULL COMMENT 'åŸºæœ¬æ¯è‚¡æ”¶ç›Š(å…ƒ)',
  diluted_eps               FLOAT NULL COMMENT 'ç¨€é‡Šæ¯è‚¡æ”¶ç›Š(å…ƒ)',
  total_revenue             DECIMAL(16,4) NULL COMMENT 'æ€»è¥æ”¶(ä¸‡å…ƒ)',
  revenue                   DECIMAL(16,4) NULL COMMENT 'è¥ä¸šæ”¶å…¥(ä¸‡å…ƒ)',
  total_cogs                DECIMAL(16,4) NULL COMMENT 'è¥ä¸šæ€»æˆæœ¬(ä¸‡å…ƒ)',
  oper_cost                 DECIMAL(16,4) NULL COMMENT 'è¥ä¸šæˆæœ¬(ä¸‡å…ƒ)',
  sell_exp                  DECIMAL(16,4) NULL COMMENT 'é”€å”®è´¹ç”¨(ä¸‡å…ƒ)',
  admin_exp                 DECIMAL(16,4) NULL COMMENT 'ç®¡ç†è´¹ç”¨(ä¸‡å…ƒ)',
  fin_exp                   DECIMAL(16,4) NULL COMMENT 'è´¢åŠ¡è´¹ç”¨(ä¸‡å…ƒ)',
  assets_impair_loss        DECIMAL(16,4) NULL COMMENT 'èµ„äº§å‡å€¼æŸå¤±(ä¸‡å…ƒ)',
  operate_profit            DECIMAL(16,4) NULL COMMENT 'è¥ä¸šåˆ©æ¶¦(ä¸‡å…ƒ)',
  non_oper_income           DECIMAL(16,4) NULL COMMENT 'è¥ä¸šå¤–æ”¶å…¥(ä¸‡å…ƒ)',
  non_oper_exp              DECIMAL(16,4) NULL COMMENT 'è¥ä¸šå¤–æ”¯å‡º(ä¸‡å…ƒ)',
  total_profit              DECIMAL(16,4) NULL COMMENT 'åˆ©æ¶¦æ€»é¢(ä¸‡å…ƒ)',
  income_tax                DECIMAL(16,4) NULL COMMENT 'æ‰€å¾—ç¨Ž(ä¸‡å…ƒ)',
  n_income                  DECIMAL(16,4) NULL COMMENT 'å‡€åˆ©æ¶¦(ä¸‡å…ƒ)',
  n_income_attr_p           DECIMAL(16,4) NULL COMMENT 'å‡€åˆ©æ¶¦(ä¸‡å…ƒ)',
  ebit                      DECIMAL(16,4) NULL COMMENT 'æ¯ç¨Žå‰åˆ©æ¶¦(ä¸‡å…ƒ)',
  ebitda                    DECIMAL(16,4) NULL COMMENT 'EBITDA(ä¸‡å…ƒ)',
  invest_income             DECIMAL(16,4) NULL COMMENT 'æŠ•èµ„æ”¶ç›Š(ä¸‡å…ƒ)',
  interest_exp              DECIMAL(16,4) NULL COMMENT 'åˆ©æ¯æ”¯å‡º(ä¸‡å…ƒ)',
  oper_exp                  DECIMAL(16,4) NULL COMMENT 'è¥ä¸šæ”¯å‡º(ä¸‡å…ƒ)',
  comshare_payable_dvd      DECIMAL(16,4) NULL COMMENT 'åº”ä»˜è‚¡åˆ©(ä¸‡å…ƒ)',
 
  -- Balance sheet fields (ä¸‡å…ƒå­˜å‚¨ - converted from å…ƒ)
  total_share               DECIMAL(16,4) NULL COMMENT 'è‚¡æœ¬(ä¸‡å…ƒ)',
  cap_rese                  DECIMAL(16,4) NULL COMMENT 'èµ„æœ¬å…¬ç§¯(ä¸‡å…ƒ)',
  undistr_porfit            DECIMAL(16,4) NULL COMMENT 'æœªåˆ†é…åˆ©æ¶¦(ä¸‡å…ƒ)',
  surplus_rese              DECIMAL(16,4) NULL COMMENT 'ç›ˆä½™å…¬ç§¯(ä¸‡å…ƒ)',
  money_cap                 DECIMAL(16,4) NULL COMMENT 'è´§å¸èµ„é‡‘(ä¸‡å…ƒ)',
  accounts_receiv           DECIMAL(16,4) NULL COMMENT 'åº”æ”¶è´¦æ¬¾(ä¸‡å…ƒ)',
  oth_receiv                DECIMAL(16,4) NULL COMMENT 'å…¶ä»–åº”æ”¶æ¬¾(ä¸‡å…ƒ)',
  prepayment                DECIMAL(16,4) NULL COMMENT 'é¢„ä»˜æ¬¾é¡¹(ä¸‡å…ƒ)',
  inventories               DECIMAL(16,4) NULL COMMENT 'å­˜è´§(ä¸‡å…ƒ)',
  oth_cur_assets            DECIMAL(16,4) NULL COMMENT 'å…¶ä»–æµåŠ¨èµ„äº§(ä¸‡å…ƒ)',
  total_cur_assets          DECIMAL(16,4) NULL COMMENT 'æµåŠ¨èµ„äº§åˆè®¡(ä¸‡å…ƒ)',
  htm_invest                DECIMAL(16,4) NULL COMMENT 'å¯ä¾›å‡ºå”®é‡‘èžèµ„äº§(ä¸‡å…ƒ)',
  fix_assets                DECIMAL(16,4) NULL COMMENT 'å›ºå®šèµ„äº§(ä¸‡å…ƒ)',
  intan_assets              DECIMAL(16,4) NULL COMMENT 'æ— å½¢èµ„äº§(ä¸‡å…ƒ)',
  defer_tax_assets          DECIMAL(16,4) NULL COMMENT 'é€’å»¶æ‰€å¾—ç¨Žèµ„äº§(ä¸‡å…ƒ)',
  total_nca                 DECIMAL(16,4) NULL COMMENT 'éžæµåŠ¨èµ„äº§åˆè®¡(ä¸‡å…ƒ)',
  total_assets              DECIMAL(16,4) NULL COMMENT 'èµ„äº§æ€»è®¡(ä¸‡å…ƒ)',
  acct_payable              DECIMAL(16,4) NULL COMMENT 'åº”ä»˜è´¦æ¬¾(ä¸‡å…ƒ)',
  payroll_payable           DECIMAL(16,4) NULL COMMENT 'åº”ä»˜èŒå·¥è–ªé…¬(ä¸‡å…ƒ)',
  taxes_payable             DECIMAL(16,4) NULL COMMENT 'åº”äº¤ç¨Žè´¹(ä¸‡å…ƒ)',
  oth_payable               DECIMAL(16,4) NULL COMMENT 'å…¶ä»–åº”ä»˜æ¬¾(ä¸‡å…ƒ)',
  total_cur_liab            DECIMAL(16,4) NULL COMMENT 'æµåŠ¨è´Ÿå€ºåˆè®¡(ä¸‡å…ƒ)',
  defer_inc_non_cur_liab    DECIMAL(16,4) NULL COMMENT 'é€’å»¶æ”¶ç›Š-éžæµåŠ¨è´Ÿå€º(ä¸‡å…ƒ)',
  total_ncl                 DECIMAL(16,4) NULL COMMENT 'éžæµåŠ¨è´Ÿå€ºåˆè®¡(ä¸‡å…ƒ)',
  total_liab                DECIMAL(16,4) NULL COMMENT 'è´Ÿå€ºåˆè®¡(ä¸‡å…ƒ)',
  total_hldr_eqy_exc_min_int DECIMAL(16,4) NULL COMMENT 'è‚¡ä¸œæƒç›Šåˆè®¡(ä¸‡å…ƒ)',
  total_hldr_eqy_inc_min_int DECIMAL(16,4) NULL COMMENT 'è‚¡ä¸œæƒç›Šåˆè®¡(å«å°‘æ•°è‚¡ä¸œ)(ä¸‡å…ƒ)',
  total_liab_hldr_eqy       DECIMAL(16,4) NULL COMMENT 'è´Ÿå€ºå’Œè‚¡ä¸œæƒç›Šæ€»è®¡(ä¸‡å…ƒ)',
  oth_pay_total             DECIMAL(16,4) NULL COMMENT 'å…¶ä»–åº”ä»˜æ¬¾æ€»è®¡(ä¸‡å…ƒ)',
  accounts_receiv_bill      DECIMAL(16,4) NULL COMMENT 'åº”æ”¶ç¥¨æ®(ä¸‡å…ƒ)',
  accounts_pay              DECIMAL(16,4) NULL COMMENT 'åº”ä»˜è´¦æ¬¾(ä¸‡å…ƒ)',
  oth_rcv_total             DECIMAL(16,4) NULL COMMENT 'å…¶ä»–åº”æ”¶æ¬¾æ€»è®¡(ä¸‡å…ƒ)',
  fix_assets_total          DECIMAL(16,4) NULL COMMENT 'å›ºå®šèµ„äº§æ€»è®¡(ä¸‡å…ƒ)',
  lt_borr                   DECIMAL(16,4) NULL COMMENT 'é•¿æœŸå€Ÿæ¬¾(ä¸‡å…ƒ)',
  st_borr                   DECIMAL(16,4) NULL COMMENT 'çŸ­æœŸå€Ÿæ¬¾(ä¸‡å…ƒ)',
  oth_eqt_tools_p_shr       DECIMAL(16,4) NULL COMMENT 'å…¶ä»–æƒç›Šå·¥å…·(ä¸‡å…ƒ)',
  r_and_d                   DECIMAL(16,4) NULL COMMENT 'ç ”å‘æ”¯å‡º(ä¸‡å…ƒ)',
  goodwill                  DECIMAL(16,4) NULL COMMENT 'å•†èª‰(ä¸‡å…ƒ)',

  -- Cash flow statement fields (ä¸‡å…ƒå­˜å‚¨ - converted from å…ƒ)
  net_profit                DECIMAL(16,4) NULL COMMENT 'å‡€åˆ©æ¶¦(ä¸‡å…ƒ)',
  finan_exp                 DECIMAL(16,4) NULL COMMENT 'è´¢åŠ¡è´¹ç”¨(ä¸‡å…ƒ)',
  c_fr_sale_sg              DECIMAL(16,4) NULL COMMENT 'é”€å”®å•†å“æ”¶æ¬¾(ä¸‡å…ƒ)',
  c_inf_fr_operate_a        DECIMAL(16,4) NULL COMMENT 'ç»è¥æ´»åŠ¨çŽ°é‡‘æµå…¥å°è®¡(ä¸‡å…ƒ)',
  c_paid_goods_s            DECIMAL(16,4) NULL COMMENT 'è´­ä¹°å•†å“ä»˜æ¬¾(ä¸‡å…ƒ)',
  c_paid_to_for_empl        DECIMAL(16,4) NULL COMMENT 'æ”¯ä»˜èŒå·¥è–ªé…¬(ä¸‡å…ƒ)',
  c_paid_for_taxes          DECIMAL(16,4) NULL COMMENT 'æ”¯ä»˜ç¨Žè´¹(ä¸‡å…ƒ)',
  n_cashflow_act            DECIMAL(16,4) NULL COMMENT 'ç»è¥æ´»åŠ¨çŽ°é‡‘æµé‡å‡€é¢(ä¸‡å…ƒ)',
  n_cashflow_inv_act        DECIMAL(16,4) NULL COMMENT 'æŠ•èµ„æ´»åŠ¨çŽ°é‡‘æµé‡å‡€é¢(ä¸‡å…ƒ)',
  free_cashflow             DECIMAL(16,4) NULL COMMENT 'è‡ªç”±çŽ°é‡‘æµ(ä¸‡å…ƒ)',
  n_cash_flows_fnc_act      DECIMAL(16,4) NULL COMMENT 'èžèµ„æ´»åŠ¨çŽ°é‡‘æµé‡å‡€é¢(ä¸‡å…ƒ)',
  n_incr_cash_cash_equ      DECIMAL(16,4) NULL COMMENT 'çŽ°é‡‘åŠçŽ°é‡‘ç­‰ä»·ç‰©å‡€å¢žåŠ é¢(ä¸‡å…ƒ)',
  c_cash_equ_beg_period     DECIMAL(16,4) NULL COMMENT 'æœŸåˆçŽ°é‡‘åŠçŽ°é‡‘ç­‰ä»·ç‰©ä½™é¢(ä¸‡å…ƒ)',
  c_cash_equ_end_period     DECIMAL(16,4) NULL COMMENT 'æœŸæœ«çŽ°é‡‘åŠçŽ°é‡‘ç­‰ä»·ç‰©ä½™é¢(ä¸‡å…ƒ)',
  im_net_cashflow_oper_act  DECIMAL(16,4) NULL COMMENT 'ç»è¥æ´»åŠ¨äº§ç”Ÿçš„çŽ°é‡‘æµé‡å‡€é¢(ä¸‡å…ƒ)',
  end_bal_cash              DECIMAL(16,4) NULL COMMENT 'æœŸæœ«çŽ°é‡‘ä½™é¢(ä¸‡å…ƒ)',
  beg_bal_cash              DECIMAL(16,4) NULL COMMENT 'æœŸåˆçŽ°é‡‘ä½™é¢(ä¸‡å…ƒ)',
  c_pay_acq_const_fiolta    DECIMAL(16,4) NULL COMMENT 'è´­å»ºå›ºå®šèµ„äº§ã€æ— å½¢èµ„äº§å’Œå…¶ä»–é•¿æœŸèµ„äº§æ”¯ä»˜çš„çŽ°é‡‘(ä¸‡å…ƒ)',
  c_disp_withdrwl_invest    DECIMAL(16,4) NULL COMMENT 'å¤„ç½®å›ºå®šèµ„äº§ã€æ— å½¢èµ„äº§å’Œå…¶ä»–é•¿æœŸèµ„äº§æ”¶å›žçš„çŽ°é‡‘å‡€é¢(ä¸‡å…ƒ)',
  c_pay_dist_dpcp_int_exp   DECIMAL(16,4) NULL COMMENT 'åˆ†é…è‚¡åˆ©ã€åˆ©æ¶¦æˆ–å¿ä»˜åˆ©æ¯æ”¯ä»˜çš„çŽ°é‡‘(ä¸‡å…ƒ)',

  -- Financial indicator fields (synchronized with tushare_validate.py + TTM extensions)
  eps                       FLOAT NULL COMMENT 'æ¯è‚¡æ”¶ç›Š(å…ƒ)',
  dt_eps                    FLOAT NULL COMMENT 'ç¨€é‡Šæ¯è‚¡æ”¶ç›Š(å…ƒ)',
  revenue_ps                FLOAT NULL COMMENT 'æ¯è‚¡è¥æ”¶(å…ƒ)',
  bps                       FLOAT NULL COMMENT 'æ¯è‚¡å‡€èµ„äº§(å…ƒ)',
  cfps                      FLOAT NULL COMMENT 'æ¯è‚¡çŽ°é‡‘æµ(å…ƒ)',
  gross_margin              FLOAT NULL COMMENT 'æ¯›åˆ©çŽ‡(%)',
  netprofit_margin          FLOAT NULL COMMENT 'å‡€åˆ©çŽ‡(%)',
  grossprofit_margin        FLOAT NULL COMMENT 'æ¯›åˆ©æ¶¦çŽ‡(%)',
  current_ratio             FLOAT NULL COMMENT 'æµåŠ¨æ¯”çŽ‡',
  quick_ratio               FLOAT NULL COMMENT 'é€ŸåŠ¨æ¯”çŽ‡',
  cash_ratio                FLOAT NULL COMMENT 'çŽ°é‡‘æ¯”çŽ‡',
  inv_turn                  FLOAT NULL COMMENT 'å­˜è´§å‘¨è½¬çŽ‡',
  ar_turn                   FLOAT NULL COMMENT 'åº”æ”¶è´¦æ¬¾å‘¨è½¬çŽ‡',
  ca_turn                   FLOAT NULL COMMENT 'æµåŠ¨èµ„äº§å‘¨è½¬çŽ‡',
  fa_turn                   FLOAT NULL COMMENT 'å›ºå®šèµ„äº§å‘¨è½¬çŽ‡',
  assets_turn               FLOAT NULL COMMENT 'æ€»èµ„äº§å‘¨è½¬çŽ‡',
  debt_to_assets            FLOAT NULL COMMENT 'èµ„äº§è´Ÿå€ºçŽ‡',
  debt_to_eqt               FLOAT NULL COMMENT 'äº§æƒæ¯”çŽ‡',
  roe                       FLOAT NULL COMMENT 'å‡€èµ„äº§æ”¶ç›ŠçŽ‡(%)',
  roa                       FLOAT NULL COMMENT 'æ€»èµ„äº§æŠ¥é…¬çŽ‡(%)',
  roic                      FLOAT NULL COMMENT 'æŠ•èµ„å›žæŠ¥çŽ‡(%)',
  netprofit_yoy             FLOAT NULL COMMENT 'å‡€åˆ©æ¶¦åŒæ¯”å¢žé•¿çŽ‡(%)',
  or_yoy                    FLOAT NULL COMMENT 'è¥ä¸šæ”¶å…¥åŒæ¯”å¢žé•¿çŽ‡(%)',
  basic_eps_yoy             FLOAT NULL COMMENT 'åŸºæœ¬æ¯è‚¡æ”¶ç›ŠåŒæ¯”å¢žé•¿çŽ‡(%)',
  assets_yoy                FLOAT NULL COMMENT 'èµ„äº§åŒæ¯”å¢žé•¿çŽ‡(%)',
  eqt_yoy                   FLOAT NULL COMMENT 'å‡€èµ„äº§åŒæ¯”å¢žé•¿çŽ‡(%)',
  ocf_yoy                   FLOAT NULL COMMENT 'ç»è¥çŽ°é‡‘æµåŒæ¯”å¢žé•¿çŽ‡(%)',
  roe_yoy                   FLOAT NULL COMMENT 'å‡€èµ„äº§æ”¶ç›ŠçŽ‡åŒæ¯”å¢žé•¿çŽ‡(%)',
  equity_yoy                FLOAT NULL COMMENT 'è‚¡ä¸œæƒç›ŠåŒæ¯”å¢žé•¿çŽ‡(%)',
  rd_exp                    FLOAT NULL COMMENT 'ç ”å‘æ”¯å‡º(ä¸‡å…ƒ)',

  -- TTM (Trailing Twelve Months) indicators - our key additions
  eps_ttm                   FLOAT NULL COMMENT 'TTMæ¯è‚¡æ”¶ç›Š(å…ƒ)',
  revenue_ps_ttm            FLOAT NULL COMMENT 'TTMæ¯è‚¡è¥æ”¶(å…ƒ)',
  ocfps_ttm                 FLOAT NULL COMMENT 'TTMæ¯è‚¡ç»è¥çŽ°é‡‘æµ(å…ƒ)',
  cfps_ttm                  FLOAT NULL COMMENT 'TTMæ¯è‚¡çŽ°é‡‘æµ(å…ƒ)',
  roe_ttm                   FLOAT NULL COMMENT 'TTMå‡€èµ„äº§æ”¶ç›ŠçŽ‡(%)',
  roa_ttm                   FLOAT NULL COMMENT 'TTMæ€»èµ„äº§æŠ¥é…¬çŽ‡(%)',
  netprofit_margin_ttm      FLOAT NULL COMMENT 'TTMå‡€åˆ©çŽ‡(%)',
  grossprofit_margin_ttm    FLOAT NULL COMMENT 'TTMæ¯›åˆ©çŽ‡(%)',
  revenue_cagr_3y           FLOAT NULL COMMENT 'è¥æ”¶ä¸‰å¹´å¤åˆå¢žé•¿çŽ‡(%)',
  netincome_cagr_3y         FLOAT NULL COMMENT 'å‡€åˆ©æ¶¦ä¸‰å¹´å¤åˆå¢žé•¿çŽ‡(%)',
  roic_ttm                  FLOAT NULL COMMENT 'TTMæŠ•èµ„å›žæŠ¥çŽ‡(%)',
  fcf_ttm                   DECIMAL(16,4) NULL COMMENT 'TTMè‡ªç”±çŽ°é‡‘æµ(ä¸‡å…ƒ)',
  fcf_margin_ttm            FLOAT NULL COMMENT 'TTMè‡ªç”±çŽ°é‡‘æµçŽ‡(%)',
  debt_to_ebitda_ttm        FLOAT NULL COMMENT 'TTMå€ºåŠ¡/EBITDAæ¯”çŽ‡',

  PRIMARY KEY (ts_code, report_period),
  INDEX idx_ann_date (ann_date),
  INDEX idx_report_period (report_period),
  INDEX idx_ts_code_ann_date (ts_code, ann_date),
  INDEX idx_ts_code_report_period_ann_date (ts_code, report_period, ann_date)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8;
"""


# === Data source field grouping definitions ===

# Base fields (shared by all data sources)
BASE_COLUMNS = ["ts_code", "ann_date", "report_period", "period", "currency"]

# Income statement fields (synchronized with tushare_validate.py)
INCOME_COLUMNS = [
    'basic_eps', 'diluted_eps', 'total_revenue', 'revenue',
    'total_cogs', 'oper_cost', 'sell_exp', 'admin_exp', 'fin_exp',
    'assets_impair_loss', 'operate_profit', 'non_oper_income', 'non_oper_exp',
    'total_profit', 'income_tax', 'n_income', 'n_income_attr_p', 'ebit',
    'ebitda', 'invest_income', 'interest_exp', 'oper_exp', 'comshare_payable_dvd'
]

# Balance sheet fields (synchronized with tushare_validate.py)
BALANCE_COLUMNS = [
    'total_share', 'cap_rese', 'undistr_porfit', 'surplus_rese', 'money_cap',
    'accounts_receiv', 'oth_receiv', 'prepayment', 'inventories',
    'oth_cur_assets', 'total_cur_assets', 'htm_invest', 'fix_assets',
    'intan_assets', 'defer_tax_assets', 'total_nca', 'total_assets',
    'acct_payable', 'payroll_payable', 'taxes_payable', 'oth_payable',
    'total_cur_liab', 'defer_inc_non_cur_liab', 'total_ncl', 'total_liab',
    'total_hldr_eqy_exc_min_int', 'total_hldr_eqy_inc_min_int',
    'total_liab_hldr_eqy', 'oth_pay_total', 'accounts_receiv_bill',
    'accounts_pay', 'oth_rcv_total', 'fix_assets_total', 'lt_borr', 'st_borr',
    'oth_eqt_tools_p_shr', 'r_and_d', 'goodwill'
]

# Cash flow statement fields (synchronized with tushare_validate.py)
CASHFLOW_COLUMNS = [
    'net_profit', 'finan_exp', 'c_fr_sale_sg', 'c_inf_fr_operate_a',
    'c_paid_goods_s', 'c_paid_to_for_empl', 'c_paid_for_taxes',
    'n_cashflow_act', 'n_cashflow_inv_act', 'free_cashflow',
    'n_cash_flows_fnc_act', 'n_incr_cash_cash_equ', 'c_cash_equ_beg_period',
    'c_cash_equ_end_period', 'im_net_cashflow_oper_act', 'end_bal_cash',
    'beg_bal_cash', 'c_pay_acq_const_fiolta', 'c_disp_withdrwl_invest',
    'c_pay_dist_dpcp_int_exp'
]

# Financial indicator fields (synchronized with tushare_validate.py + TTM extensions)
INDICATOR_COLUMNS = [
    # Core indicators from tushare_validate.py
    'eps', 'dt_eps', 'revenue_ps', 'bps', 'cfps', 'gross_margin',
    'netprofit_margin', 'grossprofit_margin', 'current_ratio', 'quick_ratio',
    'cash_ratio', 'inv_turn', 'ar_turn', 'ca_turn', 'fa_turn', 'assets_turn',
    'debt_to_assets', 'debt_to_eqt', 'roe', 'roa', 'roic', 'netprofit_yoy',
    'or_yoy', 'basic_eps_yoy', 'assets_yoy', 'eqt_yoy', 'ocf_yoy', 'roe_yoy',
    'equity_yoy', 'rd_exp',

    # TTM (Trailing Twelve Months) indicators - our key additions
    'eps_ttm', 'revenue_ps_ttm', 'ocfps_ttm', 'cfps_ttm',
    'roe_ttm', 'roa_ttm', 'netprofit_margin_ttm', 'grossprofit_margin_ttm',
    'revenue_cagr_3y', 'netincome_cagr_3y',
    'fcf_ttm', 'fcf_margin_ttm', 'debt_to_ebitda_ttm'
]

# === Data source field configuration ===

# API field name list (all three major financial statements contain these base fields)
# Note: API returns 'end_date' but database stores as 'ann_date'
API_COMMON_FIELDS = ['ts_code', 'ann_date', 'end_date', 'report_type']

# Financial indicators base fields (does not include report_type)
INDICATOR_BASE_FIELDS = ['ts_code', 'ann_date', 'end_date']  # Keep end_date for API call, will be mapped later

# === Merged total field list (used for database operations) ===
ALL_COLUMNS: List[str] = BASE_COLUMNS + INCOME_COLUMNS + BALANCE_COLUMNS + CASHFLOW_COLUMNS + INDICATOR_COLUMNS

# Fields that need conversion from å…ƒ to ä¸‡å…ƒ for storage
# These are monetary amount fields (not ratios or per-share metrics)
YUAN_TO_WAN_FIELDS = [
    # Income statement - main monetary amounts
    'total_revenue', 'revenue',
    'total_cogs', 'oper_cost', 'sell_exp', 'admin_exp', 'fin_exp',
    'assets_impair_loss', 'operate_profit', 'non_oper_income', 'non_oper_exp',
    'total_profit', 'income_tax', 'n_income', 'n_income_attr_p', 'ebit',
    'ebitda', 'invest_income', 'interest_exp', 'oper_exp', 'comshare_payable_dvd', 'rd_exp',

    # Balance sheet - main monetary amounts
    'total_share', 'cap_rese', 'undistr_porfit', 'surplus_rese', 'money_cap',
    'accounts_receiv', 'oth_receiv', 'prepayment', 'inventories',
    'oth_cur_assets', 'total_cur_assets', 'htm_invest', 'fix_assets',
    'intan_assets', 'defer_tax_assets', 'total_nca', 'total_assets',
    'acct_payable', 'payroll_payable', 'taxes_payable', 'oth_payable',
    'total_cur_liab', 'defer_inc_non_cur_liab', 'total_ncl', 'total_liab',
    'total_hldr_eqy_exc_min_int', 'total_hldr_eqy_inc_min_int',
    'total_liab_hldr_eqy', 'oth_pay_total', 'accounts_receiv_bill',
    'accounts_pay', 'oth_rcv_total', 'fix_assets_total', 'lt_borr', 'st_borr',
    'oth_eqt_tools_p_shr', 'r_and_d', 'goodwill',

    # Cash flow statement - main monetary amounts
    'net_profit', 'finan_exp', 'c_fr_sale_sg', 'c_inf_fr_operate_a',
    'c_paid_goods_s', 'c_paid_to_for_empl', 'c_paid_for_taxes',
    'n_cashflow_act', 'n_cashflow_inv_act', 'free_cashflow',
    'n_cash_flows_fnc_act', 'n_incr_cash_cash_equ', 'c_cash_equ_beg_period',
    'c_cash_equ_end_period', 'im_net_cashflow_oper_act', 'end_bal_cash',
    'beg_bal_cash', 'c_pay_acq_const_fiolta', 'c_disp_withdrwl_invest',
    'c_pay_dist_dpcp_int_exp',

    # Financial indicators - monetary amounts (not ratios)
    'extra_item', 'profit_dedt', 'op_income', 'daa', 'gross_margin',

    # Quarterly financial indicators
    'q_opincome', 'q_investincome', 'q_dtprofit',

    # Other monetary amounts
    'profit_prefin_exp', 'non_op_profit', 'fixed_assets',

    # TTM monetary amounts
    'fcf_ttm',

    # Valuation indicators
    'current_exint', 'non_current_exint', 'intrinsicvalue', 'tmv', 'lmv'
]

# Per-share metrics that should remain in å…ƒ (not converted)
PER_SHARE_FIELDS = [
    'eps', 'dt_eps', 'basic_eps', 'diluted_eps', 'q_eps',
    'bps', 'ocfps', 'retainedps', 'cfps', 'ebit_ps', 'fcff_ps', 'fcfe_ps'
]

# TTM indicator fields to add to database schema
TTM_COLUMNS = [
    # TTM basic financial indicators
    'eps_ttm', 'revenue_ps_ttm', 'ocfps_ttm', 'cfps_ttm',
    'roe_ttm', 'roa_ttm', 'netprofit_margin_ttm', 'grossprofit_margin_ttm',

    # TTM growth indicators
    'revenue_cagr_3y', 'netincome_cagr_3y',

    # TTM efficiency and quality indicators
    'fcf_ttm', 'fcf_margin_ttm', 'debt_to_ebitda_ttm'
]


def calculate_quarterly_values(group, columns):
    """Calculate quarterly values using vectorized operations within each year"""
    group = group.sort_values('report_period')
    group['year'] = group['report_period'].str[:4]
    for col in columns:
        group['q_' + col] = group.groupby('year')[col].diff().fillna(group[col])
    return group.drop(columns=['year'])


def calculate_ttm_indicators(df):
    """
    Vectorized calculation of TTM indicators.
    Assumes df has 'ts_code', 'report_period', and required columns.
    Returns df with added TTM columns.
    """
    if df.empty:
        return df

    # è½¬æ¢ä¸ºdatetimeä»¥ä¾¿å¤„ç†æ—¶é—´åºåˆ—
    df['report_date'] = pd.to_datetime(df['report_period'], format='%Y%m%d')

    # ä¸ºæ¯ä¸ªts_codeè¡¥å…¨ä¸­é—´ç¼ºå¤±çš„å­£åº¦åºåˆ—
    def complete_quarters(ts_code_group):
        ts_code, group = ts_code_group

        # æ‰¾åˆ°å®žé™…å­˜åœ¨æ•°æ®çš„æ—¥æœŸèŒƒå›´
        existing_dates = group['report_date'].dropna().sort_values()

        if len(existing_dates) < 2:
            # å¦‚æžœæ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•ç¡®å®šè¡¥å…¨èŒƒå›´ï¼Œç›´æŽ¥è¿”å›žåŽŸæ•°æ®
            group_copy = group.copy()
            group_copy['missing'] = 0  # æ ‡è®°ä¸ºéžç¼ºå¤±
            group_copy['missing_type'] = 'insufficient_data'
            return group_copy

        min_date = existing_dates.min()
        max_date = existing_dates.max()

        # ç”Ÿæˆä»Žæœ€æ—©æ•°æ®åˆ°æœ€æ™šæ•°æ®çš„å®Œæ•´å­£åº¦æœ«åºåˆ—
        full_dates = pd.date_range(start=min_date, end=max_date, freq='QE-SEP')
        full_df = pd.DataFrame({'report_date': full_dates})
        full_df['report_period'] = full_df['report_date'].dt.strftime('%Y%m%d')
        full_df['ts_code'] = ts_code  # æ·»åŠ ts_code

        # å·¦åˆå¹¶åŽŸæ•°æ®ï¼Œç¼ºå¤±å¤„NA
        merged = pd.merge(full_df, group, on=['ts_code', 'report_period', 'report_date'], how='left')

        # åˆ†æžç¼ºå¤±æ¨¡å¼
        missing_mask = merged['n_income_attr_p'].isna()
        merged['missing'] = missing_mask.astype(int)

        # è¯†åˆ«ç¼ºå¤±ç±»åž‹
        merged['missing_type'] = 'none'
        merged.loc[missing_mask, 'missing_type'] = 'data_missing'

        # è¿›ä¸€æ­¥åˆ†ç±»ç¼ºå¤±ç±»åž‹
        existing_periods = set(group['report_period'].dropna())
        if existing_periods:
            min_existing_period = min(existing_periods)
            max_existing_period = max(existing_periods)

            # ä¸¤å¤´ç¼ºå¤±ï¼šæ•°æ®èŒƒå›´å¤–çš„ç¼ºå¤±
            outside_range = (merged['report_period'] < min_existing_period) | (merged['report_period'] > max_existing_period)
            merged.loc[missing_mask & outside_range, 'missing_type'] = 'edge_missing'

            # ä¸­é—´ç¼ºå¤±ï¼šæ•°æ®èŒƒå›´å†…çš„ç¼ºå¤±
            inside_range = (merged['report_period'] >= min_existing_period) & (merged['report_period'] <= max_existing_period)
            merged.loc[missing_mask & inside_range, 'missing_type'] = 'intermediate_missing'

            # è®°å½•ä¸åŒç±»åž‹çš„ç¼ºå¤±
            intermediate_missing = merged[(merged['missing_type'] == 'intermediate_missing')]
            if not intermediate_missing.empty:
                missing_periods = intermediate_missing['report_period'].tolist()
                print(f"âš ï¸  {ts_code}: ä¸­é—´æ•°æ®ç¼ºå¤± {len(missing_periods)} ä¸ªå­£åº¦: {missing_periods}")

        # ç»Ÿè®¡ç¼ºå¤±æƒ…å†µ
        missing_stats = merged['missing_type'].value_counts()
        if missing_stats.get('intermediate_missing', 0) > 0:
            print(f"ðŸ“Š {ts_code}: ç¼ºå¤±ç»Ÿè®¡ - ä¸­é—´:{missing_stats.get('intermediate_missing', 0)}, è¾¹ç¼˜:{missing_stats.get('edge_missing', 0)}, æ•°æ®:{missing_stats.get('data_missing', 0)}")

        return merged

    # ä½¿ç”¨itertools.groupbyæ¥é¿å…pandas groupbyçš„FutureWarning
    import itertools
    df = df.sort_values(['ts_code', 'report_date'])
    groups = []
    for ts_code, group in itertools.groupby(df.iterrows(), key=lambda x: x[1]['ts_code']):
        group_df = pd.DataFrame([row[1] for row in group])
        groups.append((ts_code, group_df))

    completed_groups = [complete_quarters((ts_code, group)) for ts_code, group in groups]
    df = pd.concat(completed_groups, ignore_index=True)

    # æ™ºèƒ½å¡«å……NAæ•°æ®ï¼Œæ ¹æ®ç¼ºå¤±ç±»åž‹é‡‡ç”¨ä¸åŒç­–ç•¥
    print("ðŸ”„ å¼€å§‹æ™ºèƒ½æ•°æ®å¡«å……...")

    # 1. æµæ•°æ®ï¼ˆflow dataï¼‰ï¼šæ”¶å…¥ã€æˆæœ¬ã€çŽ°é‡‘æµç­‰
    # ä¸­é—´ç¼ºå¤±ä½¿ç”¨æ’å€¼ï¼Œä¸¤å¤´ç¼ºå¤±ä¿æŒä¸º0ï¼ˆè¡¨ç¤ºè¯¥æ—¶æœŸæ²¡æœ‰æ•°æ®ï¼‰
    flow_cols = ['n_income_attr_p', 'total_revenue', 'im_net_cashflow_oper_act']
    optional_flow_cols = ['total_cogs', 'oper_cost']
    for col in optional_flow_cols:
        if col in df.columns:
            flow_cols.append(col)

    for col in flow_cols:
        if col in df.columns:
            # å¯¹äºŽä¸­é—´ç¼ºå¤±ï¼Œä½¿ç”¨çº¿æ€§æ’å€¼
            intermediate_mask = df['missing_type'] == 'intermediate_missing'
            if intermediate_mask.any():
                # å¯¹æ¯ä¸ªè‚¡ç¥¨åˆ†åˆ«è¿›è¡Œæ’å€¼
                df[col] = df.groupby('ts_code')[col].transform(lambda x: x.interpolate(method='linear', limit_direction='both'))

            # å¯¹äºŽä¸¤å¤´ç¼ºå¤±å’Œæ•°æ®ç¼ºå¤±ï¼Œå¡«å……ä¸º0ï¼ˆè¡¨ç¤ºè¯¥æ—¶æœŸæ²¡æœ‰å‘ç”Ÿï¼‰
            edge_data_mask = (df['missing_type'] == 'edge_missing') | (df['missing_type'] == 'data_missing')
            df.loc[edge_data_mask, col] = df.loc[edge_data_mask, col].fillna(0)

    # 2. å­˜é‡æ•°æ®ï¼ˆstock dataï¼‰ï¼šèµ„äº§ã€è´Ÿå€ºã€è‚¡æƒç­‰
    # ä½¿ç”¨å‰å‘å¡«å……ï¼Œç„¶åŽåŽå‘å¡«å……ï¼Œç¡®ä¿è¿žç»­æ€§
    stock_cols = ['total_hldr_eqy_exc_min_int', 'total_assets', 'total_share']
    for col in stock_cols:
        if col in df.columns:
            # ä½¿ç”¨transformæ¥é¿å…groupbyçš„FutureWarning
            df[col] = df.groupby('ts_code')[col].transform(lambda x: x.ffill().bfill())

    # 3. ç‰¹æ®Šå¤„ç†ï¼šæŸäº›å…³é”®æŒ‡æ ‡å¦‚æžœä»ç„¶ç¼ºå¤±ï¼Œä½¿ç”¨è¡Œä¸šå¹³å‡æˆ–å…¶ä»–æ–¹æ³•
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å¡«å……é€»è¾‘

    # ç»Ÿè®¡å¡«å……ç»“æžœ
    remaining_na = df[flow_cols + stock_cols].isna().sum().sum()
    if remaining_na > 0:
        print(f"âš ï¸  ä»æœ‰ {remaining_na} ä¸ªå€¼æœªå¡«å……")
    else:
        print("âœ… æ•°æ®å¡«å……å®Œæˆï¼Œæ— å‰©ä½™ç¼ºå¤±å€¼")

    quarterly_columns = ['n_income_attr_p', 'total_revenue', 'im_net_cashflow_oper_act']
    # Add optional quarterly columns that might not exist in all datasets
    optional_quarterly_cols = ['total_cogs', 'oper_cost']
    for col in optional_quarterly_cols:
        if col in df.columns:
            quarterly_columns.append(col)

    # ä½¿ç”¨itertools.groupbyæ¥é¿å…pandas groupbyçš„FutureWarning
    df = df.sort_values(['ts_code', 'report_period'])
    groups = []
    for ts_code, group in itertools.groupby(df.iterrows(), key=lambda x: x[1]['ts_code']):
        group_df = pd.DataFrame([row[1] for row in group])
        groups.append((ts_code, group_df))

    processed_groups = [calculate_quarterly_values(group, quarterly_columns) for ts_code, group in groups]
    df = pd.concat(processed_groups, ignore_index=True)

    # Sort by ts_code and report_period
    df = df.sort_values(['ts_code', 'report_period'])

    # Calculate rolling TTM sums for quarterly values
    ttm_columns = {col: 'ttm_' + col for col in quarterly_columns}
    for q_col, ttm_col in ttm_columns.items():
        df[ttm_col] = df.groupby('ts_code')['q_' + q_col].rolling(window=4, min_periods=4).sum().reset_index(level=0, drop=True)

    # Drop rows where TTM is NaN (insufficient history)
    #df = df.dropna(subset=list(ttm_columns.values()))

    # Calculate TTM gross (only if oper_cost data is available)
    if 'ttm_oper_cost' in df.columns:
        df['ttm_gross'] = df['ttm_total_revenue'] - df['ttm_oper_cost']
    else:
        df['ttm_gross'] = df['ttm_total_revenue']  # Fallback if no cost data

    # Per-share calculations (vectorized)
    df['eps_ttm'] = np.where(df['total_share'] > 0, df['ttm_n_income_attr_p'] / df['total_share'], 0)
    df['revenue_ps_ttm'] = np.where(df['total_share'] > 0, df['ttm_total_revenue'] / df['total_share'], 0)
    df['ocfps_ttm'] = np.where(df['total_share'] > 0, df['ttm_im_net_cashflow_oper_act'] / df['total_share'], 0)
    df['cfps_ttm'] = df['ocfps_ttm']  # Assuming CFPS uses OCF

    # ROE and ROA (using period-end values)
    df['roe_ttm'] = np.where(df['total_hldr_eqy_exc_min_int'] > 0,
                             (df['ttm_n_income_attr_p'] / df['total_hldr_eqy_exc_min_int']) * 100, 0)
    df['roa_ttm'] = np.where(df['total_assets'] > 0,
                             (df['ttm_n_income_attr_p'] / df['total_assets']) * 100, 0)

    # Margins
    df['netprofit_margin_ttm'] = np.where(df['ttm_total_revenue'] > 0,
                                          (df['ttm_n_income_attr_p'] / df['ttm_total_revenue']) * 100, 0)
    # Gross margin only if cost data is available
    if 'ttm_oper_cost' in df.columns:
        df['grossprofit_margin_ttm'] = np.where(df['ttm_total_revenue'] > 0,
                                                (df['ttm_gross'] / df['ttm_total_revenue']) * 100, 0)
    else:
        df['grossprofit_margin_ttm'] = np.nan

    # CAGR (3-year, same quarter) with special handling for negative values
    df['revenue_3y_ago'] = df.groupby('ts_code')['total_revenue'].shift(12)
    df['ni_3y_ago'] = df.groupby('ts_code')['n_income_attr_p'].shift(12)

    # Revenue CAGR calculation with negative value handling
    df['revenue_cagr_3y'] = np.nan

    # Both positive (normal CAGR)
    mask_both_positive = (df['revenue_3y_ago'] > 0) & (df['total_revenue'] > 0)
    df.loc[mask_both_positive, 'revenue_cagr_3y'] = (
        (df.loc[mask_both_positive, 'total_revenue'] / df.loc[mask_both_positive, 'revenue_3y_ago']) ** (1/3) - 1
    ) * 100

    # Net Income CAGR calculation with similar logic
    df['netincome_cagr_3y'] = np.nan

    # Both positive (normal CAGR)
    mask_both_positive_ni = (df['ni_3y_ago'] > 0) & (df['n_income_attr_p'] > 0)
    df.loc[mask_both_positive_ni, 'netincome_cagr_3y'] = (
        (df.loc[mask_both_positive_ni, 'n_income_attr_p'] / df.loc[mask_both_positive_ni, 'ni_3y_ago']) ** (1/3) - 1
    ) * 100

    # FCF TTM (Free Cash Flow) - approximation using available data
    # FCF = Operating Cash Flow - CapEx
    if 'n_cashflow_act' in df.columns and 'c_pay_acq_const_fiolta' in df.columns:
        df['fcf_ttm'] = df['n_cashflow_act'] - df['c_pay_acq_const_fiolta'].fillna(0)
    elif 'n_cashflow_act' in df.columns:
        df['fcf_ttm'] = df['n_cashflow_act'] * 0.8  # Rough approximation
    else:
        df['fcf_ttm'] = np.nan

    # FCF Margin TTM
    df['fcf_margin_ttm'] = np.where(df['ttm_total_revenue'] > 0,
                                    (df['fcf_ttm'] / df['ttm_total_revenue']) * 100, np.nan)

    # Debt to EBITDA TTM ratio
    if 'total_liab' in df.columns and 'ebitda' in df.columns:
        df['debt_to_ebitda_ttm'] = np.where(df['ebitda'] > 0, df['total_liab'] / df['ebitda'], np.nan)
    else:
        df['debt_to_ebitda_ttm'] = np.nan

    # Round results
    round_cols = ['eps_ttm', 'revenue_ps_ttm', 'ocfps_ttm', 'cfps_ttm', 'roe_ttm', 'roa_ttm',
                  'netprofit_margin_ttm', 'grossprofit_margin_ttm', 'revenue_cagr_3y', 'netincome_cagr_3y',
                  'fcf_margin_ttm', 'debt_to_ebitda_ttm']
    df[round_cols] = df[round_cols].round(4)

    # Remove filled rows (missing=1) after calculations are complete
    if 'missing' in df.columns:
        original_count = len(df)
        df = df[df['missing'] != 1].copy()
        removed_count = original_count - len(df)
        if removed_count > 0:
            print(f"Removed {removed_count} filled rows after TTM/CAGR calculations")
        df = df.drop(columns=['missing'])

    return df


def convert_wan_to_yuan(df: pd.DataFrame) -> pd.DataFrame:
    """
    Convert stored ä¸‡å…ƒ values back to å…ƒ for API responses
    This ensures API consumers get data in the expected å…ƒ format

    Args:
        df: DataFrame with values stored in ä¸‡å…ƒ

    Returns:
        DataFrame with monetary values converted back to å…ƒ
    """
    if df.empty:
        return df

    df_copy = df.copy()

    # Convert ä¸‡å…ƒ back to å…ƒ for monetary amount fields
    for col in YUAN_TO_WAN_FIELDS:
        if col in df_copy.columns and df_copy[col].notna().any():
            # Convert ä¸‡å…ƒ to å…ƒ (multiply by 10,000)
            df_copy[col] = df_copy[col] * 10000.0

    return df_copy


def _coerce_schema(df: pd.DataFrame) -> pd.DataFrame:
    """Ensure all expected columns exist and normalize data types"""
    # Ensure all expected columns exist
    for col in ALL_COLUMNS:
        if col not in df.columns:
            df[col] = None

    # Keep only known columns, in order
    out = df[ALL_COLUMNS].copy()

    # Normalize types
    if not out.empty:
        # String columns (excluding date columns that will be converted to DATE type)
        string_cols = ["ts_code", "period", "currency"]
        for col in string_cols:
            if col in out.columns:
                out[col] = out[col].astype(str).replace('nan', None).replace('None', None)

        # Convert date columns from string to DATE objects for efficient storage and queries
        # This avoids SQL-level date conversion and improves insertion performance
        # Convert report_period from '2024-03-31' format to DATE object
        if 'report_period' in out.columns:
            out['report_period'] = pd.to_datetime(out['report_period'], format='%Y-%m-%d', errors='coerce').dt.date

        # Convert ann_date from '20240331' format to DATE object
        if 'ann_date' in out.columns:
            out['ann_date'] = pd.to_datetime(out['ann_date'], format='%Y%m%d', errors='coerce').dt.date

        # Numeric columns - convert to float first, then handle None values
        # Exclude string columns and date columns (report_period, ann_date)
        date_cols = ["report_period", "ann_date"]
        numeric_cols = [col for col in ALL_COLUMNS if col not in string_cols and col not in date_cols]
        for col in numeric_cols:
            if col in out.columns:
                out[col] = pd.to_numeric(out[col], errors="coerce")

        # Convert monetary amounts from å…ƒ to ä¸‡å…ƒ for storage
        # This reduces storage space and prevents DECIMAL overflow
        for col in YUAN_TO_WAN_FIELDS:
            if col in out.columns and out[col].notna().any():
                # Convert å…ƒ to ä¸‡å…ƒ (divide by 10,000)
                original_values = out[col].copy()
                out[col] = out[col] / 10000.0

                # Log conversion for large values
                large_conversions = original_values.abs() > 1000000000  # > 10äº¿
                if large_conversions.any():
                    max_original = original_values[large_conversions].max()
                    converted = out[col][large_conversions].max()
                    print(f"Converted {col}: {max_original:.0f}å…ƒ â†’ {converted:.4f}ä¸‡å…ƒ")

                print(f"Converted {col} from å…ƒ to ä¸‡å…ƒ for storage")

        # Validate and clamp numeric values to prevent DECIMAL overflow
        # After conversion to ä¸‡å…ƒ, the limits are much more generous:
        # - DECIMAL(16,4): max ~999,999,999,999ä¸‡å…ƒ (999ä¸‡äº¿), min ~-999,999,999,999ä¸‡å…ƒ
        # - DECIMAL(18,4): max ~99,999,999,999,999ä¸‡å…ƒ (99ä¸‡äº¿), min ~-99,999,999,999,999ä¸‡å…ƒ
        # - DECIMAL(22,4): max ~99,999,999,999,999,999ä¸‡å…ƒ (99ä¸‡äº¿), min ~-99,999,999,999,999,999ä¸‡å…ƒ
        decimal_limits = {
            # After å…ƒâ†’ä¸‡å…ƒ conversion, limits are very generous for most financial data
            'total_revenue': (16, 4),           # DECIMAL(16,4) - up to ~999ä¸‡äº¿ä¸‡å…ƒ
            'revenue': (16, 4),                 # DECIMAL(16,4)
            'operate_profit': (16, 4),          # DECIMAL(16,4)
            'total_profit': (16, 4),            # DECIMAL(16,4)
            'n_income_attr_p': (16, 4),         # DECIMAL(16,4)
            'basic_eps': None,                  # FLOAT - per-share metrics remain in å…ƒ
            'total_cogs': (16, 4),              # DECIMAL(16,4)
            'oper_cost': (16, 4),               # DECIMAL(16,4)
            'sell_exp': (16, 4),                # DECIMAL(16,4)
            'admin_exp': (16, 4),               # DECIMAL(16,4)
            'fin_exp': (16, 4),                 # DECIMAL(16,4)
            'invest_income': (16, 4),           # DECIMAL(16,4)
            'interest_exp': (16, 4),            # DECIMAL(16,4)
            'oper_exp': (16, 4),                # DECIMAL(16,4)
            'ebit': (16, 4),                    # DECIMAL(16,4)
            'ebitda': (16, 4),                  # DECIMAL(16,4)
            'income_tax': (16, 4),              # DECIMAL(16,4)
            'comshare_payable_dvd': (16, 4),    # DECIMAL(16,4)

            # Balance sheet fields - very generous limits after conversion
            'total_assets': (16, 4),            # DECIMAL(16,4)
            'total_liab': (16, 4),              # DECIMAL(16,4)
            'total_hldr_eqy_inc_min_int': (16, 4), # DECIMAL(16,4)
            'total_cur_assets': (16, 4),        # DECIMAL(18,4)
            'total_cur_liab': (16, 4),          # DECIMAL(18,4)
            'accounts_receiv': (16, 4),         # DECIMAL(16,4)
            'inventories': (16, 4),             # DECIMAL(16,4)
            'acct_payable': (16, 4),            # DECIMAL(16,4)
            'fix_assets': (16, 4),              # DECIMAL(16,4)
            'lt_borr': (16, 4),                 # DECIMAL(16,4)
            'r_and_d': (16, 4),                 # DECIMAL(16,4)
            'goodwill': (16, 4),                # DECIMAL(16,4)
            'intang_assets': (16, 4),           # DECIMAL(16,4)
            'st_borr': (16, 4),                 # DECIMAL(16,4)
            'total_share': (16, 4),             # DECIMAL(16,4)
            'oth_eqt_tools_p_shr': (16, 4),     # DECIMAL(16,4)

            # Cash flow fields
            'n_cashflow_act': (16, 4),          # DECIMAL(16,4)
            'n_cashflow_inv_act': (16, 4),      # DECIMAL(16,4)
            'n_cash_flows_fnc_act': (16, 4),    # DECIMAL(16,4)
            'free_cashflow': (16, 4),           # DECIMAL(16,4)
            'c_pay_acq_const_fiolta': (16, 4),  # DECIMAL(16,4)
            'c_fr_sale_sg': (16, 4),            # DECIMAL(16,4)
            'c_paid_goods_s': (16, 4),          # DECIMAL(16,4)
            'c_paid_to_for_empl': (16, 4),      # DECIMAL(16,4)
            'c_paid_for_taxes': (16, 4),        # DECIMAL(16,4)
            'n_incr_cash_cash_equ': (16, 4),    # DECIMAL(16,4)
            'c_disp_withdrwl_invest': (16, 4),  # DECIMAL(16,4)
            'c_pay_dist_dpcp_int_exp': (16, 4), # DECIMAL(16,4)
            'c_cash_equ_end_period': (16, 4),   # DECIMAL(16,4)

            # Financial indicator fields (ratios/per-share metrics - no conversion)
            'eps': None,                        # FLOAT - per-share (å…ƒ)
            'dt_eps': None,                     # FLOAT - per-share (å…ƒ)
            'gross_margin': None,               # FLOAT - ratio (%)
            'netprofit_margin': None,           # FLOAT - ratio (%)
            'grossprofit_margin': None,         # FLOAT - ratio (%)
            'ebitda_margin': None,              # FLOAT - ratio (%)
            'extra_item': (16, 4),              # DECIMAL(16,4) - monetary amount
            'profit_dedt': (16, 4),             # DECIMAL(16,4) - monetary amount
            'op_income': (16, 4),               # DECIMAL(16,4) - monetary amount
            'daa': (16, 4),                     # DECIMAL(16,4) - monetary amount
            'rd_exp': (16, 4),                  # DECIMAL(16,4) - monetary amount

            # Most other indicator fields are FLOAT (ratios) or smaller ranges
            # For safety, we'll use conservative limits for large values
        }

        # Add conservative default limits for any DECIMAL fields not explicitly defined
        # This handles cases where the database schema might differ from our assumptions
        default_decimal_limit = (16, 4)  # Conservative default: DECIMAL(16,4)

        # Check for any numeric columns that might be DECIMAL but aren't in our limits dict
        for col in numeric_cols:
            if col not in out.columns:
                continue

            if col not in decimal_limits:
                # Check if column might contain large values that could overflow
                col_values = pd.to_numeric(out[col], errors='coerce')
                if col_values.notna().any():
                    max_val = col_values.abs().max()  # Check absolute value
                    if max_val > 1000000000:  # If greater than 1 billion
                        # Apply conservative limit to prevent overflow
                        precision, scale = default_decimal_limit
                        max_allowed = 10 ** (precision - scale) - (10 ** (-scale))
                        min_allowed = -max_allowed

                        if max_val > max_allowed:
                            print(f"Warning: {col} has large value {max_val}, applying conservative limit")
                            out[col] = out[col].clip(lower=min_allowed, upper=max_allowed)

        # Apply decimal limits to prevent overflow
        for col in numeric_cols:
            if col in out.columns and decimal_limits.get(col) is not None:
                precision, scale = decimal_limits[col]
                max_value = 10 ** (precision - scale) - (10 ** (-scale))
                min_value = -max_value

                # Debug: Check for values that exceed limits before clamping
                if out[col].notna().any():
                    original_series = pd.to_numeric(out[col], errors='coerce')
                    exceeding_max = original_series > max_value
                    exceeding_min = original_series < min_value

                    if exceeding_max.any():
                        max_val = original_series[exceeding_max].max()
                        print(f"Warning: {col} has value {max_val} exceeding max {max_value}")
                    if exceeding_min.any():
                        min_val = original_series[exceeding_min].min()
                        print(f"Warning: {col} has value {min_val} below min {min_value}")

                # Clamp values to valid range
                out[col] = out[col].clip(lower=min_value, upper=max_value)

                # Log clamping results
                if out[col].notna().any():
                    final_max = pd.to_numeric(out[col], errors='coerce').max()
                    final_min = pd.to_numeric(out[col], errors='coerce').min()
                    if final_max == max_value:
                        print(f"Info: {col} clamped to max value {max_value}")
                    if final_min == min_value:
                        print(f"Info: {col} clamped to min value {min_value}")

        # Ensure DB NULLs: cast to object then replace NaN with None
        out = out.astype(object).where(pd.notna(out), None)
        # Extra safety for numpy.nan
        out = out.replace({np.nan: None})

    return out


def _fetch_single_period_data(report_period: str) -> pd.DataFrame:
    """
    Fetch financial data for a single period

    âœ¨ Optimization features:
    - Retry mechanism: Each API call retries up to 3 times with exponential backoff
    - Smart merging: Merge all data sources at once, supports partial data
    - Error recovery: Failure of one data source doesn't affect others
    - Detailed logging: Complete data fetching and merging process records
    - Simplified configuration: Field names identical to API, no mapping table needed

    Data merging strategy:
    1. Three major financial statements (Income + Balance + Cash Flow) use common keys for merging
    2. Financial indicator data uses simplified keys for merging (no report_type field)
    3. Supports partial data scenarios, returns results even with only partial data sources

    Field configuration optimization:
    - COMMON_FIELDS: Common fields for three major statements ['ts_code', 'ann_date', 'end_date', 'report_type']
    - INDICATOR_BASE_FIELDS: Base fields for financial indicators ['ts_code', 'ann_date', 'end_date']
    - Use field lists directly, no redundant mapping table

    Args:
        report_period: Report period, format like '20231231'

    Returns:
        Merged financial data DataFrame containing all available data
    """
    try:
        print(f"Fetching financial data for period: {report_period}")

        # 1. Get income statement data (with retry mechanism)
        income_fields = API_COMMON_FIELDS + INCOME_COLUMNS
        income_df = call_tushare_api_with_retry(
            pro.income_vip,
            period=report_period,
            fields=','.join(income_fields)
        )

        # 2. Get balance sheet data (with retry mechanism)
        balance_fields = API_COMMON_FIELDS + BALANCE_COLUMNS
        balance_df = call_tushare_api_with_retry(
            pro.balancesheet_vip,
            period=report_period,
            fields=','.join(balance_fields)
        )

        # 3. Get cash flow statement data (with retry mechanism)
        cashflow_fields = API_COMMON_FIELDS + CASHFLOW_COLUMNS
        cashflow_df = call_tushare_api_with_retry(
            pro.cashflow_vip,
            period=report_period,
            fields=','.join(cashflow_fields)
        )

        # 4. Get financial indicators data (with retry mechanism)
        indicator_fields = INDICATOR_BASE_FIELDS + INDICATOR_COLUMNS
        indicator_df = call_tushare_api_with_retry(
            pro.fina_indicator_vip,
            period=report_period,
            fields=','.join(indicator_fields)
        )

        # === Merge data by source grouping ===

        # Check availability of each data source
        data_sources = {
            'income_statement': income_df,
            'balance_sheet': balance_df,
            'cash_flow': cashflow_df,
            'financial_indicators': indicator_df
        }

        available_sources = {name: df for name, df in data_sources.items() if not df.empty}
        if not available_sources:
            print(f"No data available for period {report_period}")
            return pd.DataFrame()

        print(f"Available data sources for period {report_period}: {', '.join(available_sources.keys())}")

        # Debug: Print record counts for each data source
        for name, df in available_sources.items():
            if len(df) > 0:
                # Remove duplicates within each data source before merging
                # Keep the last record (potentially more updated/corrected data)
                initial_len = len(df)
                df = df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date'], keep='last')
                if len(df) < initial_len:
                    print(f"Removed {initial_len - len(df)} duplicates from {name} (kept latest)")

                available_sources[name] = df

        # Merge strategy: connect by data source grouping
        merged_df = None

        # 1. Merge three major financial statements (Income + Balance + Cash Flow)
        # These data sources have the same join keys: ['ts_code', 'ann_date', 'end_date', 'report_type']
        financial_statements = []
        if 'income_statement' in available_sources:
            financial_statements.append(available_sources['income_statement'])
        if 'balance_sheet' in available_sources:
            financial_statements.append(available_sources['balance_sheet'])
        if 'cash_flow' in available_sources:
            financial_statements.append(available_sources['cash_flow'])

        if financial_statements:
            try:
                # Start from the first data source and gradually merge other data sources
                merged_df = financial_statements[0]
                print(f"Initial merge base: {len(merged_df)} records from {list(available_sources.keys())[0]}")

                for i, df in enumerate(financial_statements[1:], 1):
                    before_count = len(merged_df)
                    source_name = list(available_sources.keys())[i]

                    # Check merge keys before merging
                    merge_keys_check = merged_df.groupby(API_COMMON_FIELDS).size()
                    incoming_keys_check = df.groupby(API_COMMON_FIELDS).size()

                    merged_df = merged_df.merge(
                        df,
                        on=API_COMMON_FIELDS,
                        how='outer'
                    )
                    after_count = len(merged_df)
                    print(f"After: {after_count} records (+{after_count - before_count})")

                print(f"Successfully merged {len(financial_statements)} financial statements: {len(merged_df)} records")
            except Exception as e:
                print(f"Error merging financial statements: {e}")
                return pd.DataFrame()

        # 2. Merge financial indicators data
        # Financial indicators data doesn't have 'report_type' field, use simplified join keys
        if merged_df is not None and 'financial_indicators' in available_sources:
            try:
                before_count = len(merged_df)
                indicator_df = available_sources['financial_indicators']

                # Check merge keys
                base_keys = merged_df.groupby(['ts_code', 'ann_date', 'end_date']).size()
                indicator_keys = indicator_df.groupby(['ts_code', 'ann_date', 'end_date']).size()

                print(f"Financial indicators merge:")
                print(f"Base data: {len(base_keys)} unique combinations, {before_count} total records")
                print(f"Indicators: {len(indicator_keys)} unique combinations, {len(indicator_df)} total records")

                merged_df = merged_df.merge(
                    indicator_df,
                    on=['ts_code', 'ann_date', 'end_date'],
                    how='left'  # Left join to ensure financial data completeness
                )
                after_count = len(merged_df)
                print(f"  Result: {after_count} records ({'+' if after_count > before_count else ''}{after_count - before_count})")
                print(f"Successfully merged financial indicators: {len(merged_df)} records total")
            except Exception as e:
                print(f"Error merging financial indicators: {e}")
                print("Continuing with financial statements only...")

        # Handle edge case: only financial indicators data, no major financial statements
        elif merged_df is None and 'financial_indicators' in available_sources:
            merged_df = available_sources['financial_indicators'].copy()
            print(f"Using financial indicators only: {len(merged_df)} records")

        if merged_df is None:
            print(f"No usable data after merging for period {report_period}")
            return pd.DataFrame()

        # Add unified fields
        try:
            merged_df['ts_code'] = merged_df['ts_code']

            # Keep the original ann_date from API - don't override with end_date
            # ann_date represents when the financial report was actually announced
            # end_date represents the end of the reporting period
            if 'ann_date' not in merged_df.columns or merged_df['ann_date'].isna().all():
                # Fallback to end_date only if ann_date is missing
                merged_df['ann_date'] = merged_df['end_date']
                print(f"Warning: Using end_date as ann_date fallback for period {report_period}")
            else:
                # Validate ann_date makes sense (should be after end_date)
                sample_ann_date = merged_df['ann_date'].iloc[0]
                sample_end_date = merged_df['end_date'].iloc[0]
                print(f"Using API ann_date: {sample_ann_date} (end_date: {sample_end_date}) for period {report_period}")

            # Convert report_period to DATE object directly from end_date
            # This creates a proper DATE object for the reporting period end date
            merged_df['report_period'] = pd.to_datetime(merged_df['end_date'], format='%Y%m%d').dt.date
            merged_df['period'] = 'annual' if report_period.endswith('1231') else 'quarter'
            merged_df['currency'] = 'CNY'  # A-share default currency is CNY

            # Remove API-specific fields that don't exist in database
            if 'end_date' in merged_df.columns:
                merged_df = merged_df.drop('end_date', axis=1)

            # Remove duplicates based on primary key (ts_code, report_period)
            # Keep the last record (potentially more updated/corrected data)
            initial_count = len(merged_df)

            # Debug: Check for duplicates before removal
            duplicate_check = merged_df.groupby(['ts_code', 'report_period']).size()
            duplicates_found = duplicate_check[duplicate_check > 1]
            if len(duplicates_found) > 0:
                print(f"Found {len(duplicates_found)} duplicate groups before removal:")
                for (ts_code, report_period), count in duplicates_found.items():
                    print(f"  {ts_code} {report_period}: {count} duplicates")

            merged_df = merged_df.drop_duplicates(subset=['ts_code', 'report_period'], keep='last')
            final_count = len(merged_df)

            if initial_count != final_count:
                print(f"Removed {initial_count - final_count} duplicate records, kept {final_count} unique records (latest)")
            else:
                print(f"No duplicates found, kept {final_count} records")

            print(f"Successfully processed {len(merged_df)} financial records for period {report_period}")
            return merged_df

        except Exception as e:
            print(f"Error processing unified fields: {e}")
            return pd.DataFrame()

    except Exception as e:
        print(f"Error in _fetch_single_period_data for period {report_period}: {e}")
        return pd.DataFrame()


def _generate_periods(end_date: str, period: str = "annual", limit: int = 1) -> List[str]:
    """Generate list of periods that need to be processed"""
    periods = []
    end_dt = datetime.datetime.strptime(end_date, "%Y%m%d")

    if period == "annual":
        # Annual reports: Get the most recent years' 12-31
        for i in range(limit):
            year = end_dt.year - i
            if datetime.datetime(year, 12, 31) <= end_dt:
                periods.append(f"{year}1231")
    else:
        # Quarterly reports: Get the most recent quarters' end dates
        quarters = [(3, 31), (6, 30), (9, 30), (12, 31)]
        current_quarter = None

        # Find current quarter
        for month, day in reversed(quarters):
            q_date = datetime.datetime(end_dt.year, month, day)
            if q_date <= end_dt:
                current_quarter = q_date
                break

        if current_quarter:
            for i in range(limit):
                periods.append(current_quarter.strftime("%Y%m%d"))
                # Calculate previous quarter
                if current_quarter.month == 3:
                    current_quarter = datetime.datetime(current_quarter.year - 1, 12, 31)
                elif current_quarter.month == 6:
                    current_quarter = datetime.datetime(current_quarter.year, 3, 31)
                elif current_quarter.month == 9:
                    current_quarter = datetime.datetime(current_quarter.year, 6, 30)
                else:  # 12
                    current_quarter = datetime.datetime(current_quarter.year, 9, 30)

    return periods


def _fetch_financial_data(end_date: str, period: str = "annual", limit: int = 1) -> pd.DataFrame:
    """Fetch financial data for multiple periods (maintain compatibility)"""
    periods = _generate_periods(end_date, period, limit)

    if not periods:
        print("No valid periods found")
        return pd.DataFrame()

    print(f"Fetching financial data for periods: {periods}")

    all_data = []
    for report_period in periods:
        df = _fetch_single_period_data(report_period)
        if not df.empty:
            all_data.append(df)

        # Add delay to avoid API limits
        time.sleep(0.5)

    if all_data:
        result_df = pd.concat(all_data, ignore_index=True)
        print(f"Successfully fetched {len(result_df)} financial records in total")
        return result_df
    else:
        return pd.DataFrame()


def _upsert_batch(engine, df: pd.DataFrame, chunksize: int = 1000) -> int:
    """Batch upsert data to MySQL

    Returns:
        int: Number of records processed (not necessarily inserted/updated)
    """
    if df is None or df.empty:
        return 0

    # Return the actual number of records in the DataFrame, not the SQL result rowcount
    # This gives a more accurate representation of processed records
    total_processed = len(df)

    meta = MetaData()
    table = Table(TABLE_NAME, meta, autoload_with=engine)

    rows = df.to_dict(orient="records")
    total_affected = 0

    with engine.begin() as conn:
        for i in range(0, len(rows), chunksize):
            batch = rows[i:i+chunksize]
            stmt = mysql_insert(table).values(batch)
            update_map: Dict[str, Any] = {
                c: getattr(stmt.inserted, c)
                for c in ALL_COLUMNS
                if c not in ("ts_code", "report_period", "ann_date")  # Exclude primary key and date fields from updates
            }
            ondup = stmt.on_duplicate_key_update(**update_map)
            result = conn.execute(ondup)
            total_affected += result.rowcount or 0

    # Log both metrics for transparency
    print(f"Processed {total_processed} records, database reported {total_affected} affected rows")
    return total_processed


def update_a_stock_financial_profile(
    mysql_url: str = "mysql+pymysql://root:@127.0.0.1:3306/investment_data",
    end_date: Optional[str] = None,
    period: str = "quarter",
    limit: int = 10,
    chunksize: int = 1000,
) -> None:
    """
    Incrementally fetch Tushare financial profile data and write to MySQL ts_a_stock_financial_profile table

    âœ¨ Optimization features:
    - Period-by-period processing: Avoid memory overflow from loading too much data at once
    - Real-time writing: Write to database immediately after processing each period
    - Memory-friendly: Release memory immediately after processing each period's data
    - ðŸ”„ Retry mechanism: Automatically retry API calls up to 3 times with exponential backoff

    Contains complete financial data, grouped by relevance:
    1. Three major financial statements: Income statement, Balance sheet, Cash flow statement
    2. Basic financial indicators: Gross margin, net margin and other basic indicators
    3. Solvency indicators: Current ratio, quick ratio, debt-to-assets ratio, etc.
    4. Operating efficiency indicators: Turnover ratios, operating efficiency, etc.
    5. Profitability indicators: ROE, ROA, net margin, etc.
    6. DuPont analysis indicators: Equity multiplier, ROE decomposition, etc.
    7. Per share indicators: Earnings per share, book value per share, etc.
    8. Cash flow indicators: Cash flow ratios, cash flow efficiency, etc.
    9. Growth indicators: Various business growth rates
    10. Quarterly financial indicators: Quarterly data and ratios
    11. Cost and expense structure analysis: Expense ratio analysis
    12. Asset structure analysis: Asset allocation and capital structure
    13. Valuation indicators: Market value, intrinsic value, etc.

    Data sources:
    - pro.income_vip() - Income statement
    - pro.balancesheet_vip() - Balance sheet
    - pro.cashflow_vip() - Cash flow statement
    - pro.fina_indicator_vip() - Financial indicators

    Data type optimization:
    - Ratios and per-share earnings: FLOAT type (precise to 6 decimal places)
    - Absolute amounts: DECIMAL(16,4) type (precise to cent)
    - Date fields: DATE type (report_period, ann_date) for efficient date operations and storage

    Field configuration optimization:
    - Field names identical to Tushare API, no mapping table needed
    - Use COMMON_FIELDS and INDICATOR_BASE_FIELDS for simplified configuration
    - Four major data source groups: Income statement, Balance sheet, Cash flow, Financial indicators

    Processing workflow:
    1. Generate list of periods that need processing
    2. Process each period individually:
       - Fetch four major data sources (three major statements + financial indicators)
       - Smart merging: Merge all available data at once
       - Error recovery: Partial data failure doesn't affect the whole process
       - Write to database immediately to free up memory
    3. Statistical processing results

    Data merging optimization:
    - One-time merging strategy: Three major statements â†’ Financial indicators
    - Smart key matching: Select appropriate join keys based on data source characteristics
    - Partial data support: Continue even if some data sources fail
    - Detailed status monitoring: Real-time display of data fetching and merging status

    Retry mechanism details:
    - Each API call failure automatically retries, up to 3 times
    - Retry intervals: 1s â†’ 2s â†’ 4s (exponential backoff)
    - Detailed recording of error information for each retry
    - Failure of one API doesn't affect other API calls

    Args:
        mysql_url: MySQL connection URL
        end_date: End date in YYYYMMDD format, defaults to yesterday
        period: Report period type, 'annual' or 'quarter'
        limit: Limit on number of report periods to fetch
        chunksize: Batch processing size
    """

    # Set end date
    if end_date is None:
        yesterday = datetime.datetime.now() - datetime.timedelta(days=1)
        end_date = yesterday.strftime("%Y%m%d")

    print(f"Starting to update financial profile data, end date: {end_date}, period type: {period}, limit: {limit}")

    # Create database engine
    engine = create_engine(mysql_url, pool_recycle=3600)

    # Create table structure
    with engine.begin() as conn:
        conn.execute(text(CREATE_TABLE_DDL))

    # Fetch and process financial profile data period by period
    print("Fetching and processing financial profile data period by period...")

    periods = _generate_periods(end_date, period, limit)
    if not periods:
        print("No valid periods found")
        return

    # Collect all data first for TTM calculations
    all_data_frames = []
    total_raw_records = 0

    for i, report_period in enumerate(periods):
        print(f"\nProcessing period {i+1}/{len(periods)}: {report_period}")

        # Get data for single period
        df = _fetch_single_period_data(report_period)

        if df.empty:
            print(f"No data retrieved for period {report_period}, skipping")
            continue

        # Data normalization
        df = _coerce_schema(df)
        all_data_frames.append(df)
        total_raw_records += len(df)

        print(f"Retrieved {len(df)} financial profile records for period {report_period}")

        # Add delay to avoid API limits (already added in _fetch_single_period_data)
        if i < len(periods) - 1:  # Not the last period, add delay
            time.sleep(0.5)

    if not all_data_frames:
        print("No data retrieved for any period")
        return

    # Combine all data for TTM calculations
    combined_df = pd.concat(all_data_frames, ignore_index=True)
    print(f"\nCombined {len(all_data_frames)} periods into {len(combined_df)} total records")

    # Calculate TTM indicators
    print("Calculating TTM (Trailing Twelve Months) indicators...")
    combined_df = calculate_ttm_indicators(combined_df)
    print(f"TTM calculation completed, {len(combined_df)} records after TTM processing")

    # Upsert to database in batches
    total_written = _upsert_batch(engine, combined_df, chunksize=chunksize)

    print(f"\nUpdate completed:")
    print(f"- Processed {len(periods)} periods")
    print(f"- Retrieved {total_raw_records} raw records")
    print(f"- Final records after TTM calculation: {len(combined_df)}")
    print(f"- Total records written to database: {total_written}")


if __name__ == "__main__":
    fire.Fire(update_a_stock_financial_profile)
