import os
import time
import datetime
import logging
from typing import Optional, List, Dict, Any

import fire
import pandas as pd
import numpy as np
import tushare as ts
from sqlalchemy import create_engine, text
from sqlalchemy import Table, MetaData
from sqlalchemy.dialects.mysql import insert as mysql_insert
import pymysql  # noqa: F401 - required by SQLAlchemy URL

from ..util import (
    setup_logging, init_tushare, call_tushare_api_with_retry
)

setup_logging(log_file='update_a_stock_financial_profile.log')
logger = logging.getLogger(__name__)

tushare_pro = init_tushare()


TABLE_NAME = "ts_a_stock_financial_profile"
CREATE_TABLE_DDL = f"""
CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
  ts_code                   VARCHAR(16)  NOT NULL,
  report_period             DATE         NOT NULL,
  period                    VARCHAR(8)   NOT NULL,
  currency                  VARCHAR(3)   NOT NULL,
  ann_date                  DATE         NULL,

  -- Income statement fields (万元存储 - converted from 元)
  basic_eps                 FLOAT NULL COMMENT '基本每股收益(元)',
  diluted_eps               FLOAT NULL COMMENT '稀释每股收益(元)',
  total_revenue             DECIMAL(16,4) NULL COMMENT '总营收(万元)',
  revenue                   DECIMAL(16,4) NULL COMMENT '营业收入(万元)',
  total_cogs                DECIMAL(16,4) NULL COMMENT '营业总成本(万元)',
  oper_cost                 DECIMAL(16,4) NULL COMMENT '营业成本(万元)',
  sell_exp                  DECIMAL(16,4) NULL COMMENT '销售费用(万元)',
  admin_exp                 DECIMAL(16,4) NULL COMMENT '管理费用(万元)',
  fin_exp                   DECIMAL(16,4) NULL COMMENT '财务费用(万元)',
  assets_impair_loss        DECIMAL(16,4) NULL COMMENT '资产减值损失(万元)',
  operate_profit            DECIMAL(16,4) NULL COMMENT '营业利润(万元)',
  non_oper_income           DECIMAL(16,4) NULL COMMENT '营业外收入(万元)',
  non_oper_exp              DECIMAL(16,4) NULL COMMENT '营业外支出(万元)',
  total_profit              DECIMAL(16,4) NULL COMMENT '利润总额(万元)',
  income_tax                DECIMAL(16,4) NULL COMMENT '所得税(万元)',
  n_income                  DECIMAL(16,4) NULL COMMENT '净利润(万元)',
  n_income_attr_p           DECIMAL(16,4) NULL COMMENT '净利润(万元)',
  ebit                      DECIMAL(16,4) NULL COMMENT '息税前利润(万元)',
  ebitda                    DECIMAL(16,4) NULL COMMENT 'EBITDA(万元)',
  invest_income             DECIMAL(16,4) NULL COMMENT '投资收益(万元)',
  interest_exp              DECIMAL(16,4) NULL COMMENT '利息支出(万元)',
  oper_exp                  DECIMAL(16,4) NULL COMMENT '营业支出(万元)',
  comshare_payable_dvd      DECIMAL(16,4) NULL COMMENT '应付股利(万元)',
 
  -- Balance sheet fields (万元存储 - converted from 元)
  total_share               DECIMAL(16,4) NULL COMMENT '股本(万元)',
  cap_rese                  DECIMAL(16,4) NULL COMMENT '资本公积(万元)',
  undistr_porfit            DECIMAL(16,4) NULL COMMENT '未分配利润(万元)',
  surplus_rese              DECIMAL(16,4) NULL COMMENT '盈余公积(万元)',
  money_cap                 DECIMAL(16,4) NULL COMMENT '货币资金(万元)',
  accounts_receiv           DECIMAL(16,4) NULL COMMENT '应收账款(万元)',
  oth_receiv                DECIMAL(16,4) NULL COMMENT '其他应收款(万元)',
  prepayment                DECIMAL(16,4) NULL COMMENT '预付款项(万元)',
  inventories               DECIMAL(16,4) NULL COMMENT '存货(万元)',
  oth_cur_assets            DECIMAL(16,4) NULL COMMENT '其他流动资产(万元)',
  total_cur_assets          DECIMAL(16,4) NULL COMMENT '流动资产合计(万元)',
  htm_invest                DECIMAL(16,4) NULL COMMENT '可供出售金融资产(万元)',
  fix_assets                DECIMAL(16,4) NULL COMMENT '固定资产(万元)',
  intan_assets              DECIMAL(16,4) NULL COMMENT '无形资产(万元)',
  defer_tax_assets          DECIMAL(16,4) NULL COMMENT '递延所得税资产(万元)',
  total_nca                 DECIMAL(16,4) NULL COMMENT '非流动资产合计(万元)',
  total_assets              DECIMAL(16,4) NULL COMMENT '资产总计(万元)',
  acct_payable              DECIMAL(16,4) NULL COMMENT '应付账款(万元)',
  payroll_payable           DECIMAL(16,4) NULL COMMENT '应付职工薪酬(万元)',
  taxes_payable             DECIMAL(16,4) NULL COMMENT '应交税费(万元)',
  oth_payable               DECIMAL(16,4) NULL COMMENT '其他应付款(万元)',
  total_cur_liab            DECIMAL(16,4) NULL COMMENT '流动负债合计(万元)',
  defer_inc_non_cur_liab    DECIMAL(16,4) NULL COMMENT '递延收益-非流动负债(万元)',
  total_ncl                 DECIMAL(16,4) NULL COMMENT '非流动负债合计(万元)',
  total_liab                DECIMAL(16,4) NULL COMMENT '负债合计(万元)',
  total_hldr_eqy_exc_min_int DECIMAL(16,4) NULL COMMENT '股东权益合计(万元)',
  total_hldr_eqy_inc_min_int DECIMAL(16,4) NULL COMMENT '股东权益合计(含少数股东)(万元)',
  total_liab_hldr_eqy       DECIMAL(16,4) NULL COMMENT '负债和股东权益总计(万元)',
  oth_pay_total             DECIMAL(16,4) NULL COMMENT '其他应付款总计(万元)',
  accounts_receiv_bill      DECIMAL(16,4) NULL COMMENT '应收票据(万元)',
  accounts_pay              DECIMAL(16,4) NULL COMMENT '应付账款(万元)',
  oth_rcv_total             DECIMAL(16,4) NULL COMMENT '其他应收款总计(万元)',
  fix_assets_total          DECIMAL(16,4) NULL COMMENT '固定资产总计(万元)',
  lt_borr                   DECIMAL(16,4) NULL COMMENT '长期借款(万元)',
  st_borr                   DECIMAL(16,4) NULL COMMENT '短期借款(万元)',
  oth_eqt_tools_p_shr       DECIMAL(16,4) NULL COMMENT '其他权益工具(万元)',
  r_and_d                   DECIMAL(16,4) NULL COMMENT '研发支出(万元)',
  goodwill                  DECIMAL(16,4) NULL COMMENT '商誉(万元)',

  -- Cash flow statement fields (万元存储 - converted from 元)
  net_profit                DECIMAL(16,4) NULL COMMENT '净利润(万元)',
  finan_exp                 DECIMAL(16,4) NULL COMMENT '财务费用(万元)',
  c_fr_sale_sg              DECIMAL(16,4) NULL COMMENT '销售商品收款(万元)',
  c_inf_fr_operate_a        DECIMAL(16,4) NULL COMMENT '经营活动现金流入小计(万元)',
  c_paid_goods_s            DECIMAL(16,4) NULL COMMENT '购买商品付款(万元)',
  c_paid_to_for_empl        DECIMAL(16,4) NULL COMMENT '支付职工薪酬(万元)',
  c_paid_for_taxes          DECIMAL(16,4) NULL COMMENT '支付税费(万元)',
  n_cashflow_act            DECIMAL(16,4) NULL COMMENT '经营活动现金流量净额(万元)',
  n_cashflow_inv_act        DECIMAL(16,4) NULL COMMENT '投资活动现金流量净额(万元)',
  free_cashflow             DECIMAL(16,4) NULL COMMENT '自由现金流(万元)',
  n_cash_flows_fnc_act      DECIMAL(16,4) NULL COMMENT '融资活动现金流量净额(万元)',
  n_incr_cash_cash_equ      DECIMAL(16,4) NULL COMMENT '现金及现金等价物净增加额(万元)',
  c_cash_equ_beg_period     DECIMAL(16,4) NULL COMMENT '期初现金及现金等价物余额(万元)',
  c_cash_equ_end_period     DECIMAL(16,4) NULL COMMENT '期末现金及现金等价物余额(万元)',
  im_net_cashflow_oper_act  DECIMAL(16,4) NULL COMMENT '经营活动产生的现金流量净额(万元)',
  end_bal_cash              DECIMAL(16,4) NULL COMMENT '期末现金余额(万元)',
  beg_bal_cash              DECIMAL(16,4) NULL COMMENT '期初现金余额(万元)',
  c_pay_acq_const_fiolta    DECIMAL(16,4) NULL COMMENT '购建固定资产、无形资产和其他长期资产支付的现金(万元)',
  c_disp_withdrwl_invest    DECIMAL(16,4) NULL COMMENT '处置固定资产、无形资产和其他长期资产收回的现金净额(万元)',
  c_pay_dist_dpcp_int_exp   DECIMAL(16,4) NULL COMMENT '分配股利、利润或偿付利息支付的现金(万元)',

  -- Financial indicator fields (synchronized with tushare_validate.py + TTM extensions)
  eps                       FLOAT NULL COMMENT '每股收益(元)',
  dt_eps                    FLOAT NULL COMMENT '稀释每股收益(元)',
  revenue_ps                FLOAT NULL COMMENT '每股营收(元)',
  bps                       FLOAT NULL COMMENT '每股净资产(元)',
  cfps                      FLOAT NULL COMMENT '每股现金流(元)',
  gross_margin              FLOAT NULL COMMENT '毛利率(%)',
  netprofit_margin          FLOAT NULL COMMENT '净利率(%)',
  grossprofit_margin        FLOAT NULL COMMENT '毛利润率(%)',
  current_ratio             FLOAT NULL COMMENT '流动比率',
  quick_ratio               FLOAT NULL COMMENT '速动比率',
  cash_ratio                FLOAT NULL COMMENT '现金比率',
  inv_turn                  FLOAT NULL COMMENT '存货周转率',
  ar_turn                   FLOAT NULL COMMENT '应收账款周转率',
  ca_turn                   FLOAT NULL COMMENT '流动资产周转率',
  fa_turn                   FLOAT NULL COMMENT '固定资产周转率',
  assets_turn               FLOAT NULL COMMENT '总资产周转率',
  debt_to_assets            FLOAT NULL COMMENT '资产负债率',
  debt_to_eqt               FLOAT NULL COMMENT '产权比率',
  roe                       FLOAT NULL COMMENT '净资产收益率(%)',
  roa                       FLOAT NULL COMMENT '总资产报酬率(%)',
  roic                      FLOAT NULL COMMENT '投资回报率(%)',
  netprofit_yoy             FLOAT NULL COMMENT '净利润同比增长率(%)',
  or_yoy                    FLOAT NULL COMMENT '营业收入同比增长率(%)',
  basic_eps_yoy             FLOAT NULL COMMENT '基本每股收益同比增长率(%)',
  assets_yoy                FLOAT NULL COMMENT '资产同比增长率(%)',
  eqt_yoy                   FLOAT NULL COMMENT '净资产同比增长率(%)',
  ocf_yoy                   FLOAT NULL COMMENT '经营现金流同比增长率(%)',
  roe_yoy                   FLOAT NULL COMMENT '净资产收益率同比增长率(%)',
  equity_yoy                FLOAT NULL COMMENT '股东权益同比增长率(%)',
  rd_exp                    FLOAT NULL COMMENT '研发支出(万元)',

  -- TTM (Trailing Twelve Months) indicators - our key additions
  eps_ttm                   FLOAT NULL COMMENT 'TTM每股收益(元)',
  revenue_ps_ttm            FLOAT NULL COMMENT 'TTM每股营收(元)',
  cfps_ttm                  FLOAT NULL COMMENT 'TTM每股现金流(元)',
  roe_ttm                   FLOAT NULL COMMENT 'TTM净资产收益率(%)',
  roa_ttm                   FLOAT NULL COMMENT 'TTM总资产报酬率(%)',
  netprofit_margin_ttm      FLOAT NULL COMMENT 'TTM净利率(%)',
  grossprofit_margin_ttm    FLOAT NULL COMMENT 'TTM毛利率(%)',
  revenue_cagr_3y           FLOAT NULL COMMENT '营收三年复合增长率(%)',
  netincome_cagr_3y         FLOAT NULL COMMENT '净利润三年复合增长率(%)',
  fcf_margin_ttm            FLOAT NULL COMMENT 'TTM自由现金流率(%)',
  debt_to_ebitda_ttm        FLOAT NULL COMMENT 'TTM债务/EBITDA比率',

  PRIMARY KEY (ts_code, report_period),
  INDEX idx_ann_date (ann_date),
  INDEX idx_report_period (report_period),
  INDEX idx_ts_code_ann_date (ts_code, ann_date),
  INDEX idx_ts_code_report_period_ann_date (ts_code, report_period, ann_date)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8;
"""


# === Data source field grouping definitions ===

# Base fields (shared by all data sources)
BASE_COLUMNS = ["ts_code", "ann_date", "report_period", "period", "currency"]

# Income statement fields (synchronized with tushare_validate.py)
INCOME_COLUMNS = [
    'basic_eps', 'diluted_eps', 'total_revenue', 'revenue',
    'total_cogs', 'oper_cost', 'sell_exp', 'admin_exp', 'fin_exp',
    'assets_impair_loss', 'operate_profit', 'non_oper_income', 'non_oper_exp',
    'total_profit', 'income_tax', 'n_income', 'n_income_attr_p', 'ebit',
    'ebitda', 'invest_income', 'interest_exp', 'oper_exp', 'comshare_payable_dvd'
]

# Balance sheet fields (synchronized with tushare_validate.py)
BALANCE_COLUMNS = [
    'total_share', 'cap_rese', 'undistr_porfit', 'surplus_rese', 'money_cap',
    'accounts_receiv', 'oth_receiv', 'prepayment', 'inventories',
    'oth_cur_assets', 'total_cur_assets', 'htm_invest', 'fix_assets',
    'intan_assets', 'defer_tax_assets', 'total_nca', 'total_assets',
    'acct_payable', 'payroll_payable', 'taxes_payable', 'oth_payable',
    'total_cur_liab', 'defer_inc_non_cur_liab', 'total_ncl', 'total_liab',
    'total_hldr_eqy_exc_min_int', 'total_hldr_eqy_inc_min_int',
    'total_liab_hldr_eqy', 'oth_pay_total', 'accounts_receiv_bill',
    'accounts_pay', 'oth_rcv_total', 'fix_assets_total', 'lt_borr', 'st_borr',
    'oth_eqt_tools_p_shr', 'r_and_d', 'goodwill'
]

# Cash flow statement fields (synchronized with tushare_validate.py)
CASHFLOW_COLUMNS = [
    'net_profit', 'finan_exp', 'c_fr_sale_sg', 'c_inf_fr_operate_a',
    'c_paid_goods_s', 'c_paid_to_for_empl', 'c_paid_for_taxes',
    'n_cashflow_act', 'n_cashflow_inv_act', 'free_cashflow',
    'n_cash_flows_fnc_act', 'n_incr_cash_cash_equ', 'c_cash_equ_beg_period',
    'c_cash_equ_end_period', 'im_net_cashflow_oper_act', 'end_bal_cash',
    'beg_bal_cash', 'c_pay_acq_const_fiolta', 'c_disp_withdrwl_invest',
    'c_pay_dist_dpcp_int_exp'
]

# Financial indicator fields (synchronized with tushare_validate.py + TTM extensions)
INDICATOR_COLUMNS = [
    # Core indicators from tushare_validate.py
    'eps', 'dt_eps', 'revenue_ps', 'bps', 'cfps', 'gross_margin',
    'netprofit_margin', 'grossprofit_margin', 'current_ratio', 'quick_ratio',
    'cash_ratio', 'inv_turn', 'ar_turn', 'ca_turn', 'fa_turn', 'assets_turn',
    'debt_to_assets', 'debt_to_eqt', 'roe', 'roa', 'roic', 'netprofit_yoy',
    'or_yoy', 'basic_eps_yoy', 'assets_yoy', 'eqt_yoy', 'ocf_yoy', 'roe_yoy',
    'equity_yoy', 'rd_exp',

    # TTM (Trailing Twelve Months) indicators - our key additions
    'eps_ttm', 'revenue_ps_ttm', 'cfps_ttm',
    'roe_ttm', 'roa_ttm', 'netprofit_margin_ttm', 'grossprofit_margin_ttm',
    'revenue_cagr_3y', 'netincome_cagr_3y',
    'fcf_margin_ttm', 'debt_to_ebitda_ttm'
]

# === Data source field configuration ===

# API field name list (all three major financial statements contain these base fields)
# Note: API returns 'end_date' but database stores as 'ann_date'
API_COMMON_FIELDS = ['ts_code', 'ann_date', 'end_date', 'report_type']

# Financial indicators base fields (does not include report_type)
INDICATOR_BASE_FIELDS = ['ts_code', 'ann_date', 'end_date']  # Keep end_date for API call, will be mapped later

# === Merged total field list (used for database operations) ===
ALL_COLUMNS: List[str] = BASE_COLUMNS + INCOME_COLUMNS + BALANCE_COLUMNS + CASHFLOW_COLUMNS + INDICATOR_COLUMNS

# Fields that need conversion from 元 to 万元 for storage
# These are monetary amount fields defined in the DDL (not ratios or per-share metrics)
YUAN_TO_WAN_FIELDS = [
    # Income statement - main monetary amounts
    'total_revenue', 'revenue',
    'total_cogs', 'oper_cost', 'sell_exp', 'admin_exp', 'fin_exp',
    'assets_impair_loss', 'operate_profit', 'non_oper_income', 'non_oper_exp',
    'total_profit', 'income_tax', 'n_income', 'n_income_attr_p', 'ebit',
    'ebitda', 'invest_income', 'interest_exp', 'oper_exp', 'comshare_payable_dvd',

    # Balance sheet - main monetary amounts
    'total_share', 'cap_rese', 'undistr_porfit', 'surplus_rese', 'money_cap',
    'accounts_receiv', 'oth_receiv', 'prepayment', 'inventories',
    'oth_cur_assets', 'total_cur_assets', 'htm_invest', 'fix_assets',
    'intan_assets', 'defer_tax_assets', 'total_nca', 'total_assets',
    'acct_payable', 'payroll_payable', 'taxes_payable', 'oth_payable',
    'total_cur_liab', 'defer_inc_non_cur_liab', 'total_ncl', 'total_liab',
    'total_hldr_eqy_exc_min_int', 'total_hldr_eqy_inc_min_int',
    'total_liab_hldr_eqy', 'oth_pay_total', 'accounts_receiv_bill',
    'accounts_pay', 'oth_rcv_total', 'fix_assets_total', 'lt_borr', 'st_borr',
    'oth_eqt_tools_p_shr', 'r_and_d', 'goodwill',

    # Cash flow statement - main monetary amounts
    'net_profit', 'finan_exp', 'c_fr_sale_sg', 'c_inf_fr_operate_a',
    'c_paid_goods_s', 'c_paid_to_for_empl', 'c_paid_for_taxes',
    'n_cashflow_act', 'n_cashflow_inv_act', 'free_cashflow',
    'n_cash_flows_fnc_act', 'n_incr_cash_cash_equ', 'c_cash_equ_beg_period',
    'c_cash_equ_end_period', 'im_net_cashflow_oper_act', 'end_bal_cash',
    'beg_bal_cash', 'c_pay_acq_const_fiolta', 'c_disp_withdrwl_invest',
    'c_pay_dist_dpcp_int_exp',

    # Financial indicators - monetary amounts (not ratios) - only rd_exp is in DDL
    'rd_exp'
]

# Per-share metrics that should remain in 元 (not converted)
PER_SHARE_FIELDS = [
    'eps', 'dt_eps', 'basic_eps', 'diluted_eps', 'q_eps',
    'bps', 'ocfps', 'retainedps', 'cfps', 'ebit_ps', 'fcff_ps', 'fcfe_ps'
]

# TTM indicator fields to add to database schema
TTM_COLUMNS = [
    # TTM basic financial indicators
    'eps_ttm', 'revenue_ps_ttm', 'cfps_ttm',
    'roe_ttm', 'roa_ttm', 'netprofit_margin_ttm', 'grossprofit_margin_ttm',

    # TTM growth indicators
    'revenue_cagr_3y', 'netincome_cagr_3y',

    # TTM efficiency and quality indicators
    'fcf_margin_ttm', 'debt_to_ebitda_ttm'
]


def calculate_quarterly_values(group, columns):
    """Calculate quarterly values using vectorized operations within each year"""
    group = group.sort_values('report_period')
    group['year'] = group['report_period'].str[:4]
    for col in columns:
        group['q_' + col] = group.groupby('year')[col].diff().fillna(group[col])
    return group.drop(columns=['year'])


def calculate_ttm_indicators(df):
    """
    Vectorized calculation of TTM indicators.
    Assumes df has 'ts_code', 'report_period', and required columns.
    Returns df with added TTM columns.
    """
    if df.empty:
        return df

    # 转换为datetime以便处理时间序列
    df['report_date'] = pd.to_datetime(df['report_period'], format='%Y%m%d')

    # 为每个ts_code补全中间缺失的季度序列
    def complete_quarters(ts_code_group):
        ts_code, group = ts_code_group

        # 找到实际存在数据的日期范围
        existing_dates = group['report_date'].dropna().sort_values()

        if len(existing_dates) < 2:
            # 如果数据点太少，无法确定补全范围，直接返回原数据
            group_copy = group.copy()
            group_copy['missing'] = 0  # 标记为非缺失
            group_copy['missing_type'] = 'insufficient_data'
            return group_copy

        min_date = existing_dates.min()
        max_date = existing_dates.max()

        # 生成从最早数据到最晚数据的完整季度末序列
        full_dates = pd.date_range(start=min_date, end=max_date, freq='QE-SEP')
        full_df = pd.DataFrame({'report_date': full_dates})
        full_df['report_period'] = full_df['report_date']
        full_df['ts_code'] = ts_code  # 添加ts_code

        # 左合并原数据，缺失处NA
        merged = pd.merge(full_df, group, on=['ts_code', 'report_period', 'report_date'], how='left')

        # 分析缺失模式
        missing_mask = merged['n_income_attr_p'].isna()
        merged['missing'] = missing_mask.astype(int)

        # 识别缺失类型
        merged['missing_type'] = 'none'
        merged.loc[missing_mask, 'missing_type'] = 'data_missing'

        # 进一步分类缺失类型
        existing_periods = set(group['report_period'].dropna())
        if existing_periods:
            min_existing_period = min(existing_periods)
            max_existing_period = max(existing_periods)

            # 两头缺失：数据范围外的缺失
            outside_range = (merged['report_period'] < min_existing_period) | (merged['report_period'] > max_existing_period)
            merged.loc[missing_mask & outside_range, 'missing_type'] = 'edge_missing'

            # 中间缺失：数据范围内的缺失
            inside_range = (merged['report_period'] >= min_existing_period) & (merged['report_period'] <= max_existing_period)
            merged.loc[missing_mask & inside_range, 'missing_type'] = 'intermediate_missing'

            # 记录不同类型的缺失
            intermediate_missing = merged[(merged['missing_type'] == 'intermediate_missing')]
            if not intermediate_missing.empty:
                missing_periods = intermediate_missing['report_period'].tolist()
                logger.warning(f"{ts_code}: 中间数据缺失 {len(missing_periods)} 个季度: {missing_periods}")

        # 统计缺失情况
        missing_stats = merged['missing_type'].value_counts()
        if missing_stats.get('intermediate_missing', 0) > 0:
            logger.info(f"{ts_code}: 缺失统计 - 中间:{missing_stats.get('intermediate_missing', 0)}, 边缘:{missing_stats.get('edge_missing', 0)}, 数据:{missing_stats.get('data_missing', 0)}")

        return merged

    # 使用itertools.groupby来避免pandas groupby的FutureWarning
    import itertools
    df = df.sort_values(['ts_code', 'report_date'])
    groups = []
    for ts_code, group in itertools.groupby(df.iterrows(), key=lambda x: x[1]['ts_code']):
        group_df = pd.DataFrame([row[1] for row in group])
        groups.append((ts_code, group_df))

    completed_groups = [complete_quarters((ts_code, group)) for ts_code, group in groups]
    df = pd.concat(completed_groups, ignore_index=True)

    # 智能填充NA数据，根据缺失类型采用不同策略
    logger.info("开始智能数据填充...")

    # 1. 流数据（flow data）：收入、成本、现金流等
    # 中间缺失使用插值，两头缺失保持为0（表示该时期没有数据）
    flow_cols = ['n_income_attr_p', 'total_revenue', 'im_net_cashflow_oper_act']
    optional_flow_cols = ['total_cogs', 'oper_cost']
    for col in optional_flow_cols:
        if col in df.columns:
            flow_cols.append(col)

    for col in flow_cols:
        if col in df.columns:
            # 对于两头缺失和数据缺失，填充为0（表示该时期没有发生）
            edge_data_mask = (df['missing_type'] == 'edge_missing') | (df['missing_type'] == 'data_missing')
            df.loc[edge_data_mask, col] = df.loc[edge_data_mask, col].fillna(0)

    # 2. 存量数据（stock data）：资产、负债、股权等
    # 使用前向填充，然后后向填充，确保连续性
    stock_cols = ['total_hldr_eqy_exc_min_int', 'total_assets', 'total_share']
    for col in stock_cols:
        if col in df.columns:
            # 使用transform来避免groupby的FutureWarning
            df[col] = df.groupby('ts_code')[col].transform(lambda x: x.ffill().bfill())

    # 3. 特殊处理：某些关键指标如果仍然缺失，使用行业平均或其他方法
    # 这里可以添加更复杂的填充逻辑

    # 统计填充结果
    remaining_na = df[flow_cols + stock_cols].isna().sum().sum()
    if remaining_na > 0:
        logger.warning(f"仍有 {remaining_na} 个值未填充")
    else:
        logger.info("数据填充完成，无剩余缺失值")

    quarterly_columns = ['n_income_attr_p', 'total_revenue', 'im_net_cashflow_oper_act']
    # Add optional quarterly columns that might not exist in all datasets
    optional_quarterly_cols = ['total_cogs', 'oper_cost']
    for col in optional_quarterly_cols:
        if col in df.columns:
            quarterly_columns.append(col)

    # 使用itertools.groupby来避免pandas groupby的FutureWarning
    df = df.sort_values(['ts_code', 'report_period'])
    groups = []
    for ts_code, group in itertools.groupby(df.iterrows(), key=lambda x: x[1]['ts_code']):
        group_df = pd.DataFrame([row[1] for row in group])
        groups.append((ts_code, group_df))

    processed_groups = [calculate_quarterly_values(group, quarterly_columns) for ts_code, group in groups]
    df = pd.concat(processed_groups, ignore_index=True)

    # Sort by ts_code and report_period
    df = df.sort_values(['ts_code', 'report_period'])

    # Calculate rolling TTM sums for quarterly values using vectorized operations
    # First sort by ts_code and report_date to ensure proper rolling window
    df = df.sort_values(['ts_code', 'report_date'])

    # For TTM calculation, we need to use the original quarterly values, not the differences
    # TTM should be sum of last 4 quarters of actual reported values
    ttm_columns = {col: 'ttm_' + col for col in quarterly_columns}
    for orig_col, ttm_col in ttm_columns.items():
        # Use rolling sum on the original quarterly values with min_periods=3
        df[ttm_col] = (
            df.groupby('ts_code')[orig_col]  # Use original column, not q_ column
            .rolling(window=4, min_periods=3)  # Allow TTM with at least 3 quarters
            .sum()
            .reset_index(level=0, drop=True)
        )

    # Drop rows where TTM is NaN (insufficient history)
    #df = df.dropna(subset=list(ttm_columns.values()))

    # Calculate TTM gross (only if oper_cost data is available)
    if 'ttm_oper_cost' in df.columns:
        df['ttm_gross'] = df['ttm_total_revenue'] - df['ttm_oper_cost']
    else:
        df['ttm_gross'] = df['ttm_total_revenue']  # Fallback if no cost data

    # Per-share calculations (vectorized)
    df['eps_ttm'] = np.where(df['total_share'] > 0, df['ttm_n_income_attr_p'] / df['total_share'], 0)
    df['revenue_ps_ttm'] = np.where(df['total_share'] > 0, df['ttm_total_revenue'] / df['total_share'], 0)
    df['cfps_ttm'] = np.where(df['total_share'] > 0, df['ttm_im_net_cashflow_oper_act'] / df['total_share'], 0)

    # ROE and ROA (using period-end values)
    df['roe_ttm'] = np.where(df['total_hldr_eqy_exc_min_int'] > 0,
                             (df['ttm_n_income_attr_p'] / df['total_hldr_eqy_exc_min_int']) * 100, 0)
    df['roa_ttm'] = np.where(df['total_assets'] > 0,
                             (df['ttm_n_income_attr_p'] / df['total_assets']) * 100, 0)

    # Margins
    df['netprofit_margin_ttm'] = np.where(df['ttm_total_revenue'] > 0,
                                          (df['ttm_n_income_attr_p'] / df['ttm_total_revenue']) * 100, 0)
    # Gross margin only if cost data is available
    if 'ttm_oper_cost' in df.columns:
        df['grossprofit_margin_ttm'] = np.where(df['ttm_total_revenue'] > 0,
                                                (df['ttm_gross'] / df['ttm_total_revenue']) * 100, 0)
    else:
        df['grossprofit_margin_ttm'] = np.nan

    # CAGR (3-year, same quarter) with special handling for negative values
    df['revenue_3y_ago'] = df.groupby('ts_code')['total_revenue'].shift(12)
    df['ni_3y_ago'] = df.groupby('ts_code')['n_income_attr_p'].shift(12)

    # Revenue CAGR calculation with negative value handling
    df['revenue_cagr_3y'] = np.nan

    # Both positive (normal CAGR)
    mask_both_positive = (df['revenue_3y_ago'] > 0) & (df['total_revenue'] > 0)
    df.loc[mask_both_positive, 'revenue_cagr_3y'] = (
        (df.loc[mask_both_positive, 'total_revenue'] / df.loc[mask_both_positive, 'revenue_3y_ago']) ** (1/3) - 1
    ) * 100

    # Net Income CAGR calculation with similar logic
    df['netincome_cagr_3y'] = np.nan

    # Both positive (normal CAGR)
    mask_both_positive_ni = (df['ni_3y_ago'] > 0) & (df['n_income_attr_p'] > 0)
    df.loc[mask_both_positive_ni, 'netincome_cagr_3y'] = (
        (df.loc[mask_both_positive_ni, 'n_income_attr_p'] / df.loc[mask_both_positive_ni, 'ni_3y_ago']) ** (1/3) - 1
    ) * 100

    # FCF TTM (Free Cash Flow) - improved calculation using historical CapEx average
    # FCF = Operating Cash Flow - CapEx (Capital Expenditures)
    if 'n_cashflow_act' in df.columns and 'c_pay_acq_const_fiolta' in df.columns:
        df['fcf_ttm'] = df['n_cashflow_act'] - df['c_pay_acq_const_fiolta'].fillna(0)
    elif 'n_cashflow_act' in df.columns:
        if 'c_pay_acq_const_fiolta' in df.columns:
            capex_avg = df.groupby('ts_code')['c_pay_acq_const_fiolta'].transform(lambda x: x.rolling(window=4, min_periods=2).mean())
            df['fcf_ttm'] = np.where(
                capex_avg.notna(),
                df['n_cashflow_act'] - capex_avg,
                df['n_cashflow_act'] * 0.7  # Or dynamically compute based on available data
            )
        else:
            df['fcf_ttm'] = df['n_cashflow_act'] * 0.7  # Pure fallback if column missing
    else:
        df['fcf_ttm'] = np.nan

    # FCF Margin TTM
    df['fcf_margin_ttm'] = np.where(df['ttm_total_revenue'] > 0,
                                    (df['fcf_ttm'] / df['ttm_total_revenue']) * 100, np.nan)

    # Debt to EBITDA TTM ratio - using net debt (total_liab - money_cap) for more accurate leverage measure
    if 'total_liab' in df.columns and 'money_cap' in df.columns and 'ebitda' in df.columns:
        net_debt = df['total_liab'] - df['money_cap']
        df['debt_to_ebitda_ttm'] = np.where(df['ebitda'] > 0, net_debt / df['ebitda'], np.nan)
    elif 'total_liab' in df.columns and 'ebitda' in df.columns:
        # Fallback to total liabilities if cash data not available
        df['debt_to_ebitda_ttm'] = np.where(df['ebitda'] > 0, df['total_liab'] / df['ebitda'], np.nan)
    else:
        df['debt_to_ebitda_ttm'] = np.nan

    # Round results
    round_cols = ['eps_ttm', 'revenue_ps_ttm', 'cfps_ttm', 'roe_ttm', 'roa_ttm',
                  'netprofit_margin_ttm', 'grossprofit_margin_ttm', 'revenue_cagr_3y', 'netincome_cagr_3y',
                  'fcf_margin_ttm', 'debt_to_ebitda_ttm']
    df[round_cols] = df[round_cols].round(4)

    # Remove filled rows (missing=1) after calculations are complete
    if 'missing' in df.columns:
        original_count = len(df)
        df = df[df['missing'] != 1].copy()
        removed_count = original_count - len(df)
        if removed_count > 0:
            logger.info(f"Removed {removed_count} filled rows after TTM/CAGR calculations")
        df = df.drop(columns=['missing'])

    return df


def _coerce_schema(df: pd.DataFrame) -> pd.DataFrame:
    """Ensure all expected columns exist and normalize data types"""
    # Ensure all expected columns exist
    for col in ALL_COLUMNS:
        if col not in df.columns:
            df[col] = None

    # Keep only known columns, in order
    out = df[ALL_COLUMNS].copy()

    # Normalize types
    if not out.empty:
        # String columns (excluding date columns that will be converted to DATE type)
        string_cols = ["ts_code", "period", "currency"]
        for col in string_cols:
            if col in out.columns:
                out[col] = out[col].astype(str).replace('nan', None).replace('None', None)

        # Convert date columns from string to DATE objects for efficient storage and queries
        # This avoids SQL-level date conversion and improves insertion performance
        # Convert report_period from '2024-03-31' format to DATE object
        if 'report_period' in out.columns:
            out['report_period'] = pd.to_datetime(out['report_period'], format='%Y-%m-%d', errors='coerce').dt.date

        # Convert ann_date from '20240331' format to DATE object
        if 'ann_date' in out.columns:
            out['ann_date'] = pd.to_datetime(out['ann_date'], format='%Y%m%d', errors='coerce').dt.date

        # Numeric columns - convert to float first, then handle None values
        # Exclude string columns and date columns (report_period, ann_date)
        date_cols = ["report_period", "ann_date"]
        numeric_cols = [col for col in ALL_COLUMNS if col not in string_cols and col not in date_cols]
        for col in numeric_cols:
            if col in out.columns:
                out[col] = pd.to_numeric(out[col], errors="coerce")

        # Convert monetary amounts from 元 to 万元 for storage
        # This reduces storage space and prevents DECIMAL overflow
        for col in YUAN_TO_WAN_FIELDS:
            if col in out.columns and out[col].notna().any():
                # Convert 元 to 万元 (divide by 10,000)
                original_values = out[col].copy()
                out[col] = out[col] / 10000.0

                # Log conversion for large values
                large_conversions = original_values.abs() > 1000000000  # > 10亿
                if large_conversions.any():
                    max_original = original_values[large_conversions].max()
                    converted = out[col][large_conversions].max()
                    logger.debug(f"Converted {col}: {max_original:.0f}元 → {converted:.4f}万元")

                logger.debug(f"Converted {col} from 元 to 万元 for storage")

        # Validate and clamp numeric values to prevent DECIMAL overflow
        # After conversion to 万元, the limits are much more generous:
        # - DECIMAL(16,4): max ~999,999,999,999万元 (999万亿), min ~-999,999,999,999万元
        # - DECIMAL(18,4): max ~99,999,999,999,999万元 (99万亿), min ~-99,999,999,999,999万元
        # - DECIMAL(22,4): max ~99,999,999,999,999,999万元 (99万亿), min ~-99,999,999,999,999,999万元
        decimal_limits = {
            # After 元→万元 conversion, limits are very generous for most financial data
            'total_revenue': (16, 4),           # DECIMAL(16,4) - up to ~999万亿万元
            'revenue': (16, 4),                 # DECIMAL(16,4)
            'operate_profit': (16, 4),          # DECIMAL(16,4)
            'total_profit': (16, 4),            # DECIMAL(16,4)
            'n_income_attr_p': (16, 4),         # DECIMAL(16,4)
            'basic_eps': None,                  # FLOAT - per-share metrics remain in 元
            'total_cogs': (16, 4),              # DECIMAL(16,4)
            'oper_cost': (16, 4),               # DECIMAL(16,4)
            'sell_exp': (16, 4),                # DECIMAL(16,4)
            'admin_exp': (16, 4),               # DECIMAL(16,4)
            'fin_exp': (16, 4),                 # DECIMAL(16,4)
            'invest_income': (16, 4),           # DECIMAL(16,4)
            'interest_exp': (16, 4),            # DECIMAL(16,4)
            'oper_exp': (16, 4),                # DECIMAL(16,4)
            'ebit': (16, 4),                    # DECIMAL(16,4)
            'ebitda': (16, 4),                  # DECIMAL(16,4)
            'income_tax': (16, 4),              # DECIMAL(16,4)
            'comshare_payable_dvd': (16, 4),    # DECIMAL(16,4)

            # Balance sheet fields - very generous limits after conversion
            'total_assets': (16, 4),            # DECIMAL(16,4)
            'total_liab': (16, 4),              # DECIMAL(16,4)
            'total_hldr_eqy_inc_min_int': (16, 4), # DECIMAL(16,4)
            'total_cur_assets': (16, 4),        # DECIMAL(18,4)
            'total_cur_liab': (16, 4),          # DECIMAL(18,4)
            'accounts_receiv': (16, 4),         # DECIMAL(16,4)
            'inventories': (16, 4),             # DECIMAL(16,4)
            'acct_payable': (16, 4),            # DECIMAL(16,4)
            'fix_assets': (16, 4),              # DECIMAL(16,4)
            'lt_borr': (16, 4),                 # DECIMAL(16,4)
            'r_and_d': (16, 4),                 # DECIMAL(16,4)
            'goodwill': (16, 4),                # DECIMAL(16,4)
            'intang_assets': (16, 4),           # DECIMAL(16,4)
            'st_borr': (16, 4),                 # DECIMAL(16,4)
            'total_share': (16, 4),             # DECIMAL(16,4)
            'oth_eqt_tools_p_shr': (16, 4),     # DECIMAL(16,4)

            # Cash flow fields
            'n_cashflow_act': (16, 4),          # DECIMAL(16,4)
            'n_cashflow_inv_act': (16, 4),      # DECIMAL(16,4)
            'n_cash_flows_fnc_act': (16, 4),    # DECIMAL(16,4)
            'free_cashflow': (16, 4),           # DECIMAL(16,4)
            'c_pay_acq_const_fiolta': (16, 4),  # DECIMAL(16,4)
            'c_fr_sale_sg': (16, 4),            # DECIMAL(16,4)
            'c_paid_goods_s': (16, 4),          # DECIMAL(16,4)
            'c_paid_to_for_empl': (16, 4),      # DECIMAL(16,4)
            'c_paid_for_taxes': (16, 4),        # DECIMAL(16,4)
            'n_incr_cash_cash_equ': (16, 4),    # DECIMAL(16,4)
            'c_disp_withdrwl_invest': (16, 4),  # DECIMAL(16,4)
            'c_pay_dist_dpcp_int_exp': (16, 4), # DECIMAL(16,4)
            'c_cash_equ_end_period': (16, 4),   # DECIMAL(16,4)

            # Financial indicator fields (ratios/per-share metrics - no conversion)
            'eps': None,                        # FLOAT - per-share (元)
            'dt_eps': None,                     # FLOAT - per-share (元)
            'gross_margin': None,               # FLOAT - ratio (%)
            'netprofit_margin': None,           # FLOAT - ratio (%)
            'grossprofit_margin': None,         # FLOAT - ratio (%)
            'ebitda_margin': None,              # FLOAT - ratio (%)
            'extra_item': (16, 4),              # DECIMAL(16,4) - monetary amount
            'profit_dedt': (16, 4),             # DECIMAL(16,4) - monetary amount
            'op_income': (16, 4),               # DECIMAL(16,4) - monetary amount
            'daa': (16, 4),                     # DECIMAL(16,4) - monetary amount
            'rd_exp': (16, 4),                  # DECIMAL(16,4) - monetary amount

            # Most other indicator fields are FLOAT (ratios) or smaller ranges
            # For safety, we'll use conservative limits for large values
        }

        # Add conservative default limits for any DECIMAL fields not explicitly defined
        # This handles cases where the database schema might differ from our assumptions
        default_decimal_limit = (16, 4)  # Conservative default: DECIMAL(16,4)

        # Check for any numeric columns that might be DECIMAL but aren't in our limits dict
        for col in numeric_cols:
            if col not in out.columns:
                continue

            if col not in decimal_limits:
                # Check if column might contain large values that could overflow
                col_values = pd.to_numeric(out[col], errors='coerce')
                if col_values.notna().any():
                    max_val = col_values.abs().max()  # Check absolute value
                    if max_val > 1000000000:  # If greater than 1 billion
                        # Apply conservative limit to prevent overflow
                        precision, scale = default_decimal_limit
                        max_allowed = 10 ** (precision - scale) - (10 ** (-scale))
                        min_allowed = -max_allowed

                        if max_val > max_allowed:
                            logger.warning(f"{col} has large value {max_val}, applying conservative limit")
                            out[col] = out[col].clip(lower=min_allowed, upper=max_allowed)

        # Apply decimal limits to prevent overflow
        for col in numeric_cols:
            if col in out.columns and decimal_limits.get(col) is not None:
                precision, scale = decimal_limits[col]
                max_value = 10 ** (precision - scale) - (10 ** (-scale))
                min_value = -max_value

                # Debug: Check for values that exceed limits before clamping
                if out[col].notna().any():
                    original_series = pd.to_numeric(out[col], errors='coerce')
                    exceeding_max = original_series > max_value
                    exceeding_min = original_series < min_value

                    if exceeding_max.any():
                        max_val = original_series[exceeding_max].max()
                        logger.warning(f"{col} has value {max_val} exceeding max {max_value}")
                    if exceeding_min.any():
                        min_val = original_series[exceeding_min].min()
                        logger.warning(f"{col} has value {min_val} below min {min_value}")

                # Clamp values to valid range
                out[col] = out[col].clip(lower=min_value, upper=max_value)

                # Log clamping results
                if out[col].notna().any():
                    final_max = pd.to_numeric(out[col], errors='coerce').max()
                    final_min = pd.to_numeric(out[col], errors='coerce').min()
                    if final_max == max_value:
                        logger.info(f"{col} clamped to max value {max_value}")
                    if final_min == min_value:
                        logger.info(f"{col} clamped to min value {min_value}")

        # Ensure DB NULLs: cast to object then replace NaN with None
        out = out.astype(object).where(pd.notna(out), None)
        # Extra safety for numpy.nan
        out = out.replace({np.nan: None})

    return out


def _fetch_single_period_data(report_period: str) -> pd.DataFrame:
    """
    Fetch financial data for a single period

    ✨ Optimization features:
    - Retry mechanism: Each API call retries up to 3 times with exponential backoff
    - Smart merging: Merge all data sources at once, supports partial data
    - Error recovery: Failure of one data source doesn't affect others
    - Detailed logging: Complete data fetching and merging process records
    - Simplified configuration: Field names identical to API, no mapping table needed

    Data merging strategy:
    1. Three major financial statements (Income + Balance + Cash Flow) use common keys for merging
    2. Financial indicator data uses simplified keys for merging (no report_type field)
    3. Supports partial data scenarios, returns results even with only partial data sources

    Field configuration optimization:
    - COMMON_FIELDS: Common fields for three major statements ['ts_code', 'ann_date', 'end_date', 'report_type']
    - INDICATOR_BASE_FIELDS: Base fields for financial indicators ['ts_code', 'ann_date', 'end_date']
    - Use field lists directly, no redundant mapping table

    Args:
        report_period: Report period, format like '20231231'

    Returns:
        Merged financial data DataFrame containing all available data
    """
    try:
        logger.info(f"Fetching financial data for period: {report_period}")

        # 1. Get income statement data (with retry mechanism)
        income_fields = API_COMMON_FIELDS + INCOME_COLUMNS
        income_df = call_tushare_api_with_retry(
            tushare_pro.income_vip,
            period=report_period,
            fields=','.join(income_fields)
        )

        # 2. Get balance sheet data (with retry mechanism)
        balance_fields = API_COMMON_FIELDS + BALANCE_COLUMNS
        balance_df = call_tushare_api_with_retry(
            tushare_pro.balancesheet_vip,
            period=report_period,
            fields=','.join(balance_fields)
        )

        # 3. Get cash flow statement data (with retry mechanism)
        cashflow_fields = API_COMMON_FIELDS + CASHFLOW_COLUMNS
        cashflow_df = call_tushare_api_with_retry(
            tushare_pro.cashflow_vip,
            period=report_period,
            fields=','.join(cashflow_fields)
        )

        # 4. Get financial indicators data (with retry mechanism)
        indicator_fields = INDICATOR_BASE_FIELDS + INDICATOR_COLUMNS
        indicator_df = call_tushare_api_with_retry(
            tushare_pro.fina_indicator_vip,
            period=report_period,
            fields=','.join(indicator_fields)
        )

        # === Merge data by source grouping ===

        # Check availability of each data source
        data_sources = {
            'income_statement': income_df,
            'balance_sheet': balance_df,
            'cash_flow': cashflow_df,
            'financial_indicators': indicator_df
        }

        available_sources = {name: df for name, df in data_sources.items() if not df.empty}
        if not available_sources:
            logger.warning(f"No data available for period {report_period}")
            return pd.DataFrame()

        logger.debug(f"Available data sources for period {report_period}: {', '.join(available_sources.keys())}")

        # Debug: Print record counts for each data source
        for name, df in available_sources.items():
            if len(df) > 0:
                # Remove duplicates within each data source before merging
                # Keep the last record (potentially more updated/corrected data)
                initial_len = len(df)
                df = df.drop_duplicates(subset=['ts_code', 'ann_date', 'end_date'], keep='last')
                if len(df) < initial_len:
                    logger.info(f"Removed {initial_len - len(df)} duplicates from {name} (kept latest)")

                available_sources[name] = df

        # Merge strategy: connect by data source grouping
        merged_df = None

        # 1. Merge three major financial statements (Income + Balance + Cash Flow)
        # These data sources have the same join keys: ['ts_code', 'ann_date', 'end_date', 'report_type']
        financial_statements = []
        if 'income_statement' in available_sources:
            financial_statements.append(available_sources['income_statement'])
        if 'balance_sheet' in available_sources:
            financial_statements.append(available_sources['balance_sheet'])
        if 'cash_flow' in available_sources:
            financial_statements.append(available_sources['cash_flow'])

        if financial_statements:
            try:
                # Start from the first data source and gradually merge other data sources
                merged_df = financial_statements[0]
                logger.debug(f"Initial merge base: {len(merged_df)} records from {list(available_sources.keys())[0]}")

                for i, df in enumerate(financial_statements[1:], 1):
                    before_count = len(merged_df)
                    source_name = list(available_sources.keys())[i]

                    # Check merge keys before merging
                    merge_keys_check = merged_df.groupby(API_COMMON_FIELDS).size()
                    incoming_keys_check = df.groupby(API_COMMON_FIELDS).size()

                    merged_df = merged_df.merge(
                        df,
                        on=API_COMMON_FIELDS,
                        how='outer'
                    )
                    after_count = len(merged_df)
                    logger.debug(f"After merging {source_name}: {after_count} records (+{after_count - before_count})")

                logger.info(f"Successfully merged {len(financial_statements)} financial statements: {len(merged_df)} records")
            except Exception as e:
                logger.error(f"Error merging financial statements: {e}")
                return pd.DataFrame()

        # 2. Merge financial indicators data
        # Financial indicators data doesn't have 'report_type' field, use simplified join keys
        if merged_df is not None and 'financial_indicators' in available_sources:
            try:
                before_count = len(merged_df)
                indicator_df = available_sources['financial_indicators']

                # Check merge keys
                base_keys = merged_df.groupby(['ts_code', 'ann_date', 'end_date']).size()
                indicator_keys = indicator_df.groupby(['ts_code', 'ann_date', 'end_date']).size()

                logger.debug(f"Financial indicators merge:")
                logger.debug(f"Base data: {len(base_keys)} unique combinations, {before_count} total records")
                logger.debug(f"Indicators: {len(indicator_keys)} unique combinations, {len(indicator_df)} total records")

                merged_df = merged_df.merge(
                    indicator_df,
                    on=['ts_code', 'ann_date', 'end_date'],
                    how='left'  # Left join to ensure financial data completeness
                )
                after_count = len(merged_df)
                logger.debug(f"  Result: {after_count} records ({'+' if after_count > before_count else ''}{after_count - before_count})")
                logger.info(f"Successfully merged financial indicators: {len(merged_df)} records total")
            except Exception as e:
                logger.error(f"Error merging financial indicators: {e}")
                logger.info("Continuing with financial statements only...")

        # Handle edge case: only financial indicators data, no major financial statements
        elif merged_df is None and 'financial_indicators' in available_sources:
            merged_df = available_sources['financial_indicators'].copy()
            logger.info(f"Using financial indicators only: {len(merged_df)} records")

        if merged_df is None:
            logger.warning(f"No usable data after merging for period {report_period}")
            return pd.DataFrame()

        # Add unified fields
        try:
            merged_df['ts_code'] = merged_df['ts_code']

            # Keep the original ann_date from API - don't override with end_date
            # ann_date represents when the financial report was actually announced
            # end_date represents the end of the reporting period
            if 'ann_date' not in merged_df.columns or merged_df['ann_date'].isna().all():
                # Fallback to end_date only if ann_date is missing
                merged_df['ann_date'] = merged_df['end_date']
                logger.warning(f"Using end_date as ann_date fallback for period {report_period}")
            else:
                # Validate ann_date makes sense (should be after end_date)
                sample_ann_date = merged_df['ann_date'].iloc[0]
                sample_end_date = merged_df['end_date'].iloc[0]
                logger.debug(f"Using API ann_date: {sample_ann_date} (end_date: {sample_end_date}) for period {report_period}")

            # Convert report_period to DATE object directly from end_date
            # This creates a proper DATE object for the reporting period end date
            merged_df['report_period'] = pd.to_datetime(merged_df['end_date'], format='%Y%m%d').dt.date
            merged_df['period'] = 'annual' if report_period.endswith('1231') else 'quarter'
            merged_df['currency'] = 'CNY'  # A-share default currency is CNY

            # Remove API-specific fields that don't exist in database
            if 'end_date' in merged_df.columns:
                merged_df = merged_df.drop('end_date', axis=1)

            # Remove duplicates based on primary key (ts_code, report_period)
            # Keep the last record (potentially more updated/corrected data)
            initial_count = len(merged_df)

            # Debug: Check for duplicates before removal
            duplicate_check = merged_df.groupby(['ts_code', 'report_period']).size()
            duplicates_found = duplicate_check[duplicate_check > 1]
            if len(duplicates_found) > 0:
                logger.warning(f"Found {len(duplicates_found)} duplicate groups before removal:")
                for (ts_code, report_period), count in duplicates_found.items():
                    logger.warning(f"  {ts_code} {report_period}: {count} duplicates")

            merged_df = merged_df.drop_duplicates(subset=['ts_code', 'report_period'], keep='last')
            final_count = len(merged_df)

            if initial_count != final_count:
                logger.info(f"Removed {initial_count - final_count} duplicate records, kept {final_count} unique records (latest)")
            else:
                logger.debug(f"No duplicates found, kept {final_count} records")

            logger.info(f"Successfully processed {len(merged_df)} financial records for period {report_period}")
            return merged_df

        except Exception as e:
            logger.error(f"Error processing unified fields: {e}")
            return pd.DataFrame()

    except Exception as e:
        logger.error(f"Error in _fetch_single_period_data for period {report_period}: {e}")
        return pd.DataFrame()


def _generate_periods(end_date: str, period: str = "annual", limit: int = 1) -> List[str]:
    """Generate list of periods that need to be processed"""
    periods = []
    end_dt = datetime.datetime.strptime(end_date, "%Y%m%d")

    if period == "annual":
        # Annual reports: Get the most recent years' 12-31
        for i in range(limit):
            year = end_dt.year - i
            if datetime.datetime(year, 12, 31) <= end_dt:
                periods.append(f"{year}1231")
    else:
        # Quarterly reports: Get the most recent quarters' end dates
        quarters = [(3, 31), (6, 30), (9, 30), (12, 31)]
        current_quarter = None

        # Find current quarter
        for month, day in reversed(quarters):
            q_date = datetime.datetime(end_dt.year, month, day)
            if q_date <= end_dt:
                current_quarter = q_date
                break

        if current_quarter:
            for i in range(limit):
                periods.append(current_quarter.strftime("%Y%m%d"))
                # Calculate previous quarter
                if current_quarter.month == 3:
                    current_quarter = datetime.datetime(current_quarter.year - 1, 12, 31)
                elif current_quarter.month == 6:
                    current_quarter = datetime.datetime(current_quarter.year, 3, 31)
                elif current_quarter.month == 9:
                    current_quarter = datetime.datetime(current_quarter.year, 6, 30)
                else:  # 12
                    current_quarter = datetime.datetime(current_quarter.year, 9, 30)

    return periods


def _fetch_financial_data(end_date: str, period: str = "annual", limit: int = 1) -> pd.DataFrame:
    """Fetch financial data for multiple periods (maintain compatibility)"""
    periods = _generate_periods(end_date, period, limit)

    if not periods:
        logger.warning("No valid periods found")
        return pd.DataFrame()

    logger.info(f"Fetching financial data for periods: {periods}")

    all_data = []
    for report_period in periods:
        df = _fetch_single_period_data(report_period)
        if not df.empty:
            all_data.append(df)

        # Add delay to avoid API limits
        time.sleep(0.5)

    if all_data:
        result_df = pd.concat(all_data, ignore_index=True)
        logger.info(f"Successfully fetched {len(result_df)} financial records in total")
        return result_df
    else:
        return pd.DataFrame()


def _upsert_batch(engine, df: pd.DataFrame, chunksize: int = 1000) -> int:
    """Batch upsert data to MySQL

    Returns:
        int: Number of records processed (not necessarily inserted/updated)
    """
    if df is None or df.empty:
        return 0

    # Return the actual number of records in the DataFrame, not the SQL result rowcount
    # This gives a more accurate representation of processed records
    total_processed = len(df)

    meta = MetaData()
    table = Table(TABLE_NAME, meta, autoload_with=engine)

    rows = df.to_dict(orient="records")
    total_affected = 0

    with engine.begin() as conn:
        for i in range(0, len(rows), chunksize):
            batch = rows[i:i+chunksize]
            stmt = mysql_insert(table).values(batch)
            update_map: Dict[str, Any] = {
                c: getattr(stmt.inserted, c)
                for c in ALL_COLUMNS
                if c not in ("ts_code", "report_period", "ann_date")  # Exclude primary key and date fields from updates
            }
            ondup = stmt.on_duplicate_key_update(**update_map)
            result = conn.execute(ondup)
            total_affected += result.rowcount or 0

    # Log both metrics for transparency
    logger.info(f"Processed {total_processed} records, database reported {total_affected} affected rows")
    return total_processed


def update_a_stock_financial_profile(
    mysql_url: str = "mysql+pymysql://root:@127.0.0.1:3306/investment_data",
    end_date: Optional[str] = None,
    period: str = "quarter",
    limit: int = 10,
    chunksize: int = 1000,
) -> None:
    """
    Incrementally fetch Tushare financial profile data and write to MySQL ts_a_stock_financial_profile table

    ✨ Optimization features:
    - Period-by-period processing: Avoid memory overflow from loading too much data at once
    - Real-time writing: Write to database immediately after processing each period
    - Memory-friendly: Release memory immediately after processing each period's data
    - 🔄 Retry mechanism: Automatically retry API calls up to 3 times with exponential backoff
    - 🛡️ Global error handling: Catch database and API exceptions with graceful degradation

    Contains complete financial data, grouped by relevance:
    1. Three major financial statements: Income statement, Balance sheet, Cash flow statement
    2. Basic financial indicators: Gross margin, net margin and other basic indicators
    3. Solvency indicators: Current ratio, quick ratio, debt-to-assets ratio, etc.
    4. Operating efficiency indicators: Turnover ratios, operating efficiency, etc.
    5. Profitability indicators: ROE, ROA, net margin, etc.
    6. DuPont analysis indicators: Equity multiplier, ROE decomposition, etc.
    7. Per share indicators: Earnings per share, book value per share, etc.
    8. Cash flow indicators: Cash flow ratios, cash flow efficiency, etc.
    9. Growth indicators: Various business growth rates
    10. Quarterly financial indicators: Quarterly data and ratios
    11. Cost and expense structure analysis: Expense ratio analysis
    12. Asset structure analysis: Asset allocation and capital structure
    13. Valuation indicators: Market value, intrinsic value, etc.

    Data sources:
    - pro.income_vip() - Income statement
    - pro.balancesheet_vip() - Balance sheet
    - pro.cashflow_vip() - Cash flow statement
    - pro.fina_indicator_vip() - Financial indicators

    Data type optimization:
    - Ratios and per-share earnings: FLOAT type (precise to 6 decimal places)
    - Absolute amounts: DECIMAL(16,4) type (precise to cent)
    - Date fields: DATE type (report_period, ann_date) for efficient date operations and storage

    Field configuration optimization:
    - Field names identical to Tushare API, no mapping table needed
    - Use COMMON_FIELDS and INDICATOR_BASE_FIELDS for simplified configuration
    - Four major data source groups: Income statement, Balance sheet, Cash flow, Financial indicators

    Processing workflow:
    1. Generate list of periods that need processing
    2. Process each period individually:
       - Fetch four major data sources (three major statements + financial indicators)
       - Smart merging: Merge all available data at once
       - Error recovery: Partial data failure doesn't affect the whole process
       - Write to database immediately to free up memory
    3. Statistical processing results

    Data merging optimization:
    - One-time merging strategy: Three major statements → Financial indicators
    - Smart key matching: Select appropriate join keys based on data source characteristics
    - Partial data support: Continue even if some data sources fail
    - Detailed status monitoring: Real-time display of data fetching and merging status

    Retry mechanism details:
    - Each API call failure automatically retries, up to 3 times
    - Retry intervals: 1s → 2s → 4s (exponential backoff)
    - Detailed recording of error information for each retry
    - Failure of one API doesn't affect other API calls

    Args:
        mysql_url: MySQL connection URL
        end_date: End date in YYYYMMDD format, defaults to yesterday
        period: Report period type, 'annual' or 'quarter'
        limit: Limit on number of report periods to fetch
        chunksize: Batch processing size
    """
    try:
        # Set end date
        if end_date is None:
            yesterday = datetime.datetime.now() - datetime.timedelta(days=1)
            end_date = yesterday.strftime("%Y%m%d")

        logger.info(f"Starting to update financial profile data, end date: {end_date}, period type: {period}, limit: {limit}")

        # Create database engine
        engine = create_engine(mysql_url, pool_recycle=3600)

        # Create table structure
        with engine.begin() as conn:
            conn.execute(text(CREATE_TABLE_DDL))

        # Fetch and process financial profile data period by period
        logger.info("Fetching and processing financial profile data period by period...")

        periods = _generate_periods(end_date, period, limit)
        if not periods:
            logger.warning("No valid periods found")
            return

        # Collect all data first for TTM calculations
        all_data_frames = []
        total_raw_records = 0

        for i, report_period in enumerate(periods):
            logger.info(f"Processing period {i+1}/{len(periods)}: {report_period}")

            # Get data for single period
            df = _fetch_single_period_data(report_period)

            if df.empty:
                logger.warning(f"No data retrieved for period {report_period}, skipping")
                continue

            # Data normalization
            df = _coerce_schema(df)
            all_data_frames.append(df)
            total_raw_records += len(df)

            logger.debug(f"Retrieved {len(df)} financial profile records for period {report_period}")

            # Add delay to avoid API limits (already added in _fetch_single_period_data)
            if i < len(periods) - 1:  # Not the last period, add delay
                time.sleep(0.5)

        if not all_data_frames:
            logger.warning("No data retrieved for any period")
            return

        # Combine all data for TTM calculations
        combined_df = pd.concat(all_data_frames, ignore_index=True)
        logger.info(f"Combined {len(all_data_frames)} periods into {len(combined_df)} total records")

        # Calculate TTM indicators
        logger.info("Calculating TTM (Trailing Twelve Months) indicators...")
        combined_df = calculate_ttm_indicators(combined_df)
        logger.info(f"TTM calculation completed, {len(combined_df)} records after TTM processing")

        # Upsert to database in batches
        total_written = _upsert_batch(engine, combined_df, chunksize=chunksize)

        logger.info(f"Update completed:")
        logger.info(f"- Processed {len(periods)} periods")
        logger.info(f"- Retrieved {total_raw_records} raw records")
        logger.info(f"- Final records after TTM calculation: {len(combined_df)}")
        logger.info(f"- Total records written to database: {total_written}")
    except Exception as e:
        logger.error(f"Fatal error in update_a_stock_financial_profile: {e}")
        logger.error(f"Error details: {type(e).__name__}: {str(e)}")
        raise  # Re-raise to maintain original error behavior


if __name__ == "__main__":
    fire.Fire(update_a_stock_financial_profile)
